{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Data science project template","text":"<p>A modern template for data science projects with all the necessary tools for experiment, development, testing, and deployment. From notebooks to production.</p> <p>\u2728\ud83d\udcda\u2728 Documentation: https://joserzapata.github.io/data-science-project-template/</p> <p>Source Code: https://github.com/JoseRZapata/data-science-project-template</p>"},{"location":"#creating-a-new-project","title":"\ud83d\udcc1 Creating a New Project","text":""},{"location":"#recommendations","title":"\ud83d\udc4d Recommendations","text":"<p>It is highly recommended to use a python version manager like Pyenv and this project is set to use Poetry &gt;= 1.8 to manage the dependencies and the environment.</p> <p>Note: Poetry &gt;= 1.8 should always be installed in a dedicated virtual environment to isolate it from the rest of your system. why?</p> <p>\ud83c\udf1f Check how to setup your environment: https://joserzapata.github.io/data-science-project-template/local_setup/</p>"},{"location":"#via-cruft-recommended","title":"\ud83c\udf6a\ud83e\udd47 Via Cruft - recommended","text":"install cruft<pre><code>pip install --user cruft # Install `cruft` on your path for easy access\n</code></pre> create project<pre><code>cruft create https://github.com/JoseRZapata/data-science-project-template\n</code></pre>"},{"location":"#via-cookiecutter","title":"\ud83c\udf6a Via Cookiecutter","text":"install cookiecutter<pre><code>pip install --user cookiecutter # Install `cookiecutter` on your path for easy access\n</code></pre> create project<pre><code>cookiecutter gh:JoseRZapata/data-science-project-template\n</code></pre> <p>Note: Cookiecutter uses <code>gh:</code> as short-hand for <code>https://github.com/</code></p>"},{"location":"#linking-an-existing-project","title":"\ud83d\udd17  Linking an Existing Project","text":"<p>If the project was originally installed via Cookiecutter, you must first use Cruft to link the project with the original template:</p> <pre><code>cruft link https://github.com/JoseRZapata/data-science-project-template\n</code></pre> <p>Then/else:</p> <pre><code>cruft update\n</code></pre>"},{"location":"#project-structure","title":"\ud83d\uddc3\ufe0f Project structure","text":"<p>Folder structure for data science projects  why?</p> <p>Data structure</p> <pre><code>.\n\u251c\u2500\u2500 .code_quality\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 mypy.ini                        # mypy configuration\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 ruff.toml                       # ruff configuration\n\u251c\u2500\u2500 .github                             # github configuration\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 actions\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 python-poetry-env\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0     \u2514\u2500\u2500 action.yml              # github action to setup python environment\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 dependabot.md                   # github action to update dependencies\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 pull_request_template.md        # template for pull requests\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 workflows                       # github actions workflows\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 ci.yml                      # run continuous integration (tests, pre-commit, etc.)\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 dependency_review.yml       # review dependencies\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 docs.yml                    # build documentation (mkdocs)\n\u2502\u00a0\u00a0     \u2514\u2500\u2500 pre-commit_autoupdate.yml   # update pre-commit hooks\n\u251c\u2500\u2500 .vscode                             # vscode configuration\n|   \u251c\u2500\u2500 extensions.json                 # list of recommended extensions\n|   \u251c\u2500\u2500 launch.json                     # vscode launch configuration\n|   \u2514\u2500\u2500 settings.json                   # vscode settings\n\u251c\u2500\u2500 conf                                # folder configuration files\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 config.yaml                     # main configuration file\n\u251c\u2500\u2500 data\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 01_raw                          # raw immutable data\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 02_intermediate                 # typed data\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 03_primary                      # domain model data\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 04_feature                      # model features\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 05_model_input                  # often called 'master tables'\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 06_models                       # serialized models\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 07_model_output                 # data generated by model runs\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 08_reporting                    # reports, results, etc\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 README.md                       # description of the data structure\n\u251c\u2500\u2500 docs                                # documentation for your project\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 index.md                        # documentation homepage\n\u251c\u2500\u2500 models                              # store final models\n\u251c\u2500\u2500 notebooks\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 1-data                          # data extraction and cleaning\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 2-exploration                   # exploratory data analysis (EDA)\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 3-analysis                      # Statistical analysis, hypothesis testing.\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 4-feat_eng                      # feature engineering (creation, selection, and transformation.)\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 5-models                        # model training, experimentation, and hyperparameter tuning.\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 6-evaluation                    # evaluation metrics, performance assessment\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 7-deploy                        # model packaging, deployment strategies.\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 8-reports                       # story telling, summaries and analysis conclusions.\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 notebook_template.ipynb         # template for notebooks\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 README.md                       # information about the notebooks\n\u251c\u2500\u2500 src                                 # source code for use in this project\n\u2502   \u251c\u2500\u2500 libs                            # custom python scripts\n\u2502   \u2502   \u251c\u2500\u2500 data_etl                    # data extraction, transformation, and loading  \n\u2502   \u2502   \u251c\u2500\u2500 data_validation             # data validation  \n\u2502   \u2502   \u251c\u2500\u2500 feat_cleaning               # feature engineering data cleaning\n\u2502   \u2502   \u251c\u2500\u2500 feat_encoding               # feature engineering encoding\n\u2502   \u2502   \u251c\u2500\u2500 feat_imputation             # feature engineering imputation    \n\u2502   \u2502   \u251c\u2500\u2500 feat_new_features           # feature engineering new features\n\u2502   \u2502   \u251c\u2500\u2500 feat_pipelines              # feature engineering pipelines\n\u2502   \u2502   \u251c\u2500\u2500 feat_preprocess_strings     # feature engineering pre process strings\n\u2502   \u2502   \u251c\u2500\u2500 feat_scaling                # feature engineering scaling data\n\u2502   \u2502   \u251c\u2500\u2500 feat_selection              # feature engineering feature selection\n\u2502   \u2502   \u251c\u2500\u2500 feat_strings                # feature engineering strings\n\u2502   \u2502   \u251c\u2500\u2500 metrics                     # evaluation metrics\n\u2502   \u2502   \u251c\u2500\u2500 model                       # model training and prediction    \n\u2502   \u2502   \u251c\u2500\u2500 model_evaluation            # model evaluation\n\u2502   \u2502   \u251c\u2500\u2500 model_selection             # model selection\n\u2502   \u2502   \u251c\u2500\u2500 model_validation            # model validation\n\u2502   \u2502   \u2514\u2500\u2500 reports                     # reports\n\u2502   \u251c\u2500\u2500 pipelines\n\u2502   \u2502   \u251c\u2500\u2500 data_etl                    # data extraction, transformation, and loading\n\u2502   \u2502   \u251c\u2500\u2500 feature_engineering         # prepare data for modeling\n\u2502   \u2502   \u251c\u2500\u2500 model_evaluation            # evaluate model performance\n\u2502   \u2502   \u251c\u2500\u2500 model_prediction            # model predictions\n\u2502   \u2502   \u2514\u2500\u2500 model_train                 # train models    \n\u251c\u2500\u2500 tests                               # test code for your project\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 test_mock.py                    # example test file\n\u251c\u2500\u2500 .editorconfig                       # editor configuration\n\u251c\u2500\u2500 .gitignore                          # files to ignore in git\n\u251c\u2500\u2500 .pre-commit-config.yaml             # configuration for pre-commit hooks\n\u251c\u2500\u2500 codecov.yml                         # configuration for codecov\n\u251c\u2500\u2500 Makefile                            # useful commands to setup environment, run tests, etc.\n\u251c\u2500\u2500 mkdocs.yml                          # configuration for mkdocs documentation\n\u251c\u2500\u2500 poetry.toml                         # poetry virtual environment configuration\n\u251c\u2500\u2500 pyproject.toml                      # dependencies for poetry\n\u2514\u2500\u2500 README.md                           # description of your project    \n</code></pre>"},{"location":"#features-and-tools","title":"\u2728 Features and Tools","text":""},{"location":"#project-standardization-and-automation","title":"\ud83d\ude80 Project Standardization and Automation","text":""},{"location":"#developer-workflow-automation","title":"\ud83d\udd28 Developer Workflow Automation","text":"<ul> <li>Python packaging, dependency management and environment management   with Poetry - <code>why?</code></li> <li>Project workflow orchestration   with Make as an interface shim<ul> <li>Self-documenting Makefile; just type   <code>make</code> on the command line to display auto-generated documentation on available   targets:</li> </ul> </li> <li>Automated Cookiecutter template synchronization with Cruft - <code>why?</code></li> <li>Code quality tooling automation and management with pre-commit</li> <li>Continuous integration and deployment with GitHub Actions</li> <li>Project configuration files  with Hydra - <code>why?</code></li> </ul>"},{"location":"#conditionally-rendered-python-package-or-project-boilerplate","title":"\ud83c\udf31 Conditionally Rendered Python Package or Project Boilerplate","text":"<ul> <li>Optional: Jupyter support</li> </ul>"},{"location":"#maintainability","title":"\ud83d\udd27 Maintainability","text":""},{"location":"#type-checking-and-data-validation","title":"\ud83c\udff7\ufe0f  Type Checking and Data Validation","text":"<ul> <li>Static type-checking with Mypy</li> </ul>"},{"location":"#testingcoverage","title":"\u2705 \ud83e\uddea Testing/Coverage","text":"<ul> <li>Testing with Pytest</li> <li>Code coverage with Coverage.py</li> <li>Coverage reporting with Codecov</li> </ul>"},{"location":"#linting","title":"\ud83d\udea8 Linting","text":""},{"location":"#code-quality","title":"\ud83d\udd0d Code quality","text":"<ul> <li>Ruff An extremely fast (10x-100x faster) Python linter and code formatter, written in Rust.<ul> <li>Replacement for Pylint, Flake8 (including major plugins) and more linters under a single, common interface</li> </ul> </li> <li>ShellCheck</li> <li>Unsanitary commits:<ul> <li>Secrets with <code>detect-secrets</code></li> <li>Large files with <code>check-added-large-files</code></li> <li>Files that contain merge conflict strings.check-merge-conflict</li> </ul> </li> </ul>"},{"location":"#code-formatting","title":"\ud83c\udfa8 Code formatting","text":"<ul> <li> <p>Ruff An extremely fast (10x-100x faster) Python linter and code formatter, written in Rust.</p> <ul> <li>Replacement for Black, isort, pyupgrade and more formatters under a single, common interface</li> </ul> </li> <li> <p>General file formatting:</p> <ul> <li><code>end-of-file-fixer</code></li> <li><code>pretty-format-json</code></li> <li>(trim) <code>trailing-whitespace</code></li> <li><code>check-yaml</code></li> </ul> </li> </ul>"},{"location":"#cicd","title":"\ud83d\udc77 CI/CD","text":""},{"location":"#automatic-dependency-updates","title":"Automatic Dependency updates","text":"<ul> <li> <p>Dependency updates with Dependabot, Automated Dependabot PR merging with the Dependabot Auto Merge GitHub Action</p> </li> <li> <p>This is a replacement for pip-audit , In your local environment, If you want to check for vulnerabilities in your dependencies you can use [pip-audit].</p> </li> </ul>"},{"location":"#dependency-review-in-pr","title":"Dependency Review in PR","text":"<ul> <li>Dependency Review  with dependency-review-action, This action scans your pull requests for dependency changes, and will raise an error if any vulnerabilities or invalid licenses are being introduced.</li> </ul>"},{"location":"#pre-commit-automatic-updates","title":"Pre-commit automatic updates","text":"<ul> <li>Automatic updates with GitHub Actions workflow <code>.github/workflows/pre-commit_autoupdate.yml</code></li> </ul>"},{"location":"#security","title":"\ud83d\udd12 Security","text":""},{"location":"#static-application-security-testing-sast","title":"\ud83d\udd0f Static Application Security Testing (SAST)","text":"<ul> <li>Code vulnerabilities with Bandit using Ruff</li> </ul>"},{"location":"#accessibility","title":"\u2328\ufe0f Accessibility","text":""},{"location":"#automation-tool-makefile","title":"\ud83d\udd28 Automation tool (Makefile)","text":"<p>Makefile to automate the setup of your environment, the installation of dependencies, the execution of tests, etc. in terminal type <code>make</code> to see the available commands</p> <pre><code>Target                Description\n-------------------   ----------------------------------------------------\ncheck                 Run code quality tools with pre-commit hooks.\ndocs_test             Test if documentation can be built without warnings or errors\ndocs_view             Build and serve the documentation\ninit_env              Install dependencies with poetry and activate env\ninit_git              Initialize git repository\ninstall_data_libs     Install pandas, scikit-learn, Jupyter, seaborn\ninstall_mlops_libs    Install dvc, mlflow\npre-commit_update     Update pre-commit hooks\ntest                  Test the code with pytest and coverage\n</code></pre>"},{"location":"#project-documentation","title":"\ud83d\udcdd Project Documentation","text":"<ul> <li>Documentation building   with MkDocs - Tutorial<ul> <li>Powered by mkdocs-material</li> <li>Rich automatic documentation from type annotations and docstrings (NumPy, Google, etc.) with mkdocstrings</li> </ul> </li> </ul>"},{"location":"#templates","title":"\ud83d\uddc3\ufe0f Templates","text":"<ul> <li>Pull Request template</li> <li>Notebook template</li> </ul>"},{"location":"#references","title":"References","text":"<ul> <li>https://drivendata.github.io/cookiecutter-data-science/</li> <li>https://github.com/crmne/cookiecutter-modern-datascience</li> <li>https://github.com/fpgmaas/cookiecutter-poetry</li> <li>https://github.com/khuyentran1401/data-science-template</li> <li>https://github.com/woltapp/wolt-python-package-cookiecutter</li> <li>https://khuyentran1401.github.io/reproducible-data-science/structure_project/introduction.html</li> <li>https://github.com/TeoZosa/cookiecutter-cruft-poetry-tox-pre-commit-ci-cd</li> <li>https://github.com/cjolowicz/cookiecutter-hypermodern-python</li> <li>https://github.com/gotofritz/cookiecutter-gotofritz-poetry</li> <li>https://github.com/kedro-org/kedro-starters</li> </ul>"},{"location":"data_schema/","title":"Data structure","text":"<p>layered data-engineering convention</p> <p></p> <code>Folder in data</code> <code>Description</code> <code>raw</code> initial start of the pipeline, containing the sourced data model(s) that should never be changed, it forms your single source of truth to work from. these data models are typically un-typed in most cases e.g. csv, but this will vary from case to case <code>intermediate</code> optional data model(s), which are introduced to type your raw data model(s), e.g. converting string based values into their current typed representation <code>primary</code> domain specific data model(s) containing cleansed, transformed and wrangled data from either raw or intermediate, which forms your layer that you input into your feature engineering <code>feature</code> analytics specific data model(s) containing a set of features defined against the primary data, which are grouped by feature area of analysis and stored against a common dimension <code>model input</code> analytics specific data model(s) containing all feature data against a common dimension and in the case of live projects against an analytics run date to ensure that you track the historical changes of the features over time <code>models</code> stored, serialised pre-trained machine learning models <code>model output</code> analytics specific data model(s) containing the results generated by the model based on the model input data <code>reporting</code> reporting data model(s) that are used to combine a set of primary, feature, model input and model output data used to drive the dashboard and the views constructed. it encapsulates and removes the need to define any blending or joining of data, improve performance and replacement of presentation layer without having to redefine the data models"},{"location":"data_schema/#references","title":"References","text":"<ul> <li>https://docs.kedro.org/en/0.18.6/faq/faq.html#what-is-data-engineering-convention</li> <li>https://towardsdatascience.com/the-importance-of-layered-thinking-in-data-engineering-a09f685edc71</li> </ul>"},{"location":"directory_hierarchy/","title":"\ud83d\uddc2\ufe0f Directory Hierarchy","text":"<p>```bash . \u251c\u2500\u2500 .code_quality \u2502\u00a0\u00a0 \u251c\u2500\u2500 mypy.ini                        # mypy configuration \u2502\u00a0\u00a0 \u2514\u2500\u2500 ruff.toml                       # ruff configuration \u251c\u2500\u2500 .github                             # github configuration \u2502\u00a0\u00a0 \u251c\u2500\u2500 actions \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 python-poetry-env \u2502\u00a0\u00a0 \u2502\u00a0\u00a0     \u2514\u2500\u2500 action.yml              # github action to setup python environment \u2502\u00a0\u00a0 \u251c\u2500\u2500 dependabot.md                   # github action to update dependencies \u2502\u00a0\u00a0 \u251c\u2500\u2500 pull_request_template.md        # template for pull requests \u2502\u00a0\u00a0 \u2514\u2500\u2500 workflows                       # github actions workflows \u2502\u00a0\u00a0     \u251c\u2500\u2500 ci.yml                      # run continuous integration (tests, pre-commit, etc.) \u2502\u00a0\u00a0     \u251c\u2500\u2500 dependency_review.yml       # review dependencies \u2502\u00a0\u00a0     \u251c\u2500\u2500 docs.yml                    # build documentation (mkdocs) \u2502\u00a0\u00a0     \u2514\u2500\u2500 pre-commit_autoupdate.yml   # update pre-commit hooks \u251c\u2500\u2500 .vscode                             # vscode configuration |   \u251c\u2500\u2500 extensions.json                 # list of recommended extensions |   \u251c\u2500\u2500 launch.json                     # vscode launch configuration |   \u2514\u2500\u2500 settings.json                   # vscode settings \u251c\u2500\u2500 conf                                # folder configuration files \u2502\u00a0\u00a0 \u2514\u2500\u2500 config.yaml                     # main configuration file \u251c\u2500\u2500 data \u2502\u00a0\u00a0 \u251c\u2500\u2500 01_raw                          # raw immutable data \u2502\u00a0\u00a0 \u251c\u2500\u2500 02_intermediate                 # typed data \u2502\u00a0\u00a0 \u251c\u2500\u2500 03_primary                      # domain model data \u2502\u00a0\u00a0 \u251c\u2500\u2500 04_feature                      # model features \u2502\u00a0\u00a0 \u251c\u2500\u2500 05_model_input                  # often called 'master tables' \u2502\u00a0\u00a0 \u251c\u2500\u2500 06_models                       # serialized models \u2502\u00a0\u00a0 \u251c\u2500\u2500 07_model_output                 # data generated by model runs \u2502\u00a0\u00a0 \u251c\u2500\u2500 08_reporting                    # reports, results, etc \u2502\u00a0\u00a0 \u2514\u2500\u2500 README.md                       # description of the data structure \u251c\u2500\u2500 docs                                # documentation for your project \u2502\u00a0\u00a0 \u251c\u2500\u2500 index.md                        # documentation homepage \u251c\u2500\u2500 models                              # store final models \u251c\u2500\u2500 notebooks \u2502\u00a0\u00a0 \u251c\u2500\u2500 1-data                          # data extraction and cleaning \u2502\u00a0\u00a0 \u251c\u2500\u2500 2-exploration                   # exploratory data analysis (EDA) \u2502\u00a0\u00a0 \u251c\u2500\u2500 3-analysis                      # Statistical analysis, hypothesis testing. \u2502\u00a0\u00a0 \u251c\u2500\u2500 4-feat_eng                      # feature engineering (creation, selection, and transformation.) \u2502\u00a0\u00a0 \u251c\u2500\u2500 5-models                        # model training, experimentation, and hyperparameter tuning. \u2502\u00a0\u00a0 \u251c\u2500\u2500 6-evaluation                    # evaluation metrics, performance assessment \u2502\u00a0\u00a0 \u251c\u2500\u2500 7-deploy                        # model packaging, deployment strategies. \u2502\u00a0\u00a0 \u251c\u2500\u2500 8-reports                       # story telling, summaries and analysis conclusions. \u2502\u00a0\u00a0 \u251c\u2500\u2500 notebook_template.ipynb         # template for notebooks \u2502\u00a0\u00a0 \u2514\u2500\u2500 README.md                       # information about the notebooks \u251c\u2500\u2500 src                                 # source code for use in this project \u2502   \u251c\u2500\u2500 libs                            # custom python scripts \u2502   \u2502   \u251c\u2500\u2500 data_etl                    # data extraction, transformation, and loading \u2502   \u2502   \u251c\u2500\u2500 data_validation             # data validation \u2502   \u2502   \u251c\u2500\u2500 feat_cleaning               # feature engineering data cleaning \u2502   \u2502   \u251c\u2500\u2500 feat_encoding               # feature engineering encoding \u2502   \u2502   \u251c\u2500\u2500 feat_imputation             # feature engineering imputation   \u2502   \u2502   \u251c\u2500\u2500 feat_new_features           # feature engineering new features \u2502   \u2502   \u251c\u2500\u2500 feat_pipelines              # feature engineering pipelines \u2502   \u2502   \u251c\u2500\u2500 feat_preprocess_strings     # feature engineering pre process strings \u2502   \u2502   \u251c\u2500\u2500 feat_scaling                # feature engineering scaling data \u2502   \u2502   \u251c\u2500\u2500 feat_selection              # feature engineering feature selection \u2502   \u2502   \u251c\u2500\u2500 feat_strings                # feature engineering strings \u2502   \u2502   \u251c\u2500\u2500 metrics                     # evaluation metrics \u2502   \u2502   \u251c\u2500\u2500 model                       # model training and prediction   \u2502   \u2502   \u251c\u2500\u2500 model_evaluation            # model evaluation \u2502   \u2502   \u251c\u2500\u2500 model_selection             # model selection \u2502   \u2502   \u251c\u2500\u2500 model_validation            # model validation \u2502   \u2502   \u2514\u2500\u2500 reports                     # reports \u2502   \u251c\u2500\u2500 pipelines \u2502   \u2502   \u251c\u2500\u2500 data_etl                    # data extraction, transformation, and loading \u2502   \u2502   \u251c\u2500\u2500 feature_engineering         # prepare data for modeling \u2502   \u2502   \u251c\u2500\u2500 model_evaluation            # evaluate model performance \u2502   \u2502   \u251c\u2500\u2500 model_prediction            # model predictions \u2502   \u2502   \u2514\u2500\u2500 model_train                 # train models   \u251c\u2500\u2500 tests                               # test code for your project \u2502\u00a0\u00a0 \u2514\u2500\u2500 test_mock.py                    # example test file \u251c\u2500\u2500 .editorconfig                       # editor configuration \u251c\u2500\u2500 .gitignore                          # files to ignore in git \u251c\u2500\u2500 .pre-commit-config.yaml             # configuration for pre-commit hooks \u251c\u2500\u2500 codecov.yml                         # configuration for codecov \u251c\u2500\u2500 Makefile                            # useful commands to setup environment, run tests, etc.</p>"},{"location":"local_setup/","title":"\ud83d\udee0\ufe0f Local Dev environment setup","text":"<p>I develop data science python projects in Linux OS or MAC OS. (For Windows OS I recommend WSL and run commands as Linux OS).</p> <p>I setup my local development environment using the following steps:</p>"},{"location":"local_setup/#computer-setup-to-develop-with-python","title":"\ud83d\udcbb Computer setup to develop with Python","text":"<ol> <li>Install Git<ul> <li>Linux: <code>sudo apt-get install git</code></li> <li>MAC: <code>brew install git</code></li> </ul> </li> <li>Install Make<ul> <li>Linux: <code>sudo apt-get install make</code></li> <li>MAC: <code>brew install make</code></li> </ul> </li> <li>Install Pyenv and after install set up the terminal for Pyenv - Link to set up<ul> <li>Linux: <code>curl https://pyenv.run | bash</code></li> <li>MAC: <code>brew install pyenv</code></li> <li>Check the installation version executing in terminal: <code>pyenv --version</code> for help go to pyenv installation help</li> </ul> </li> <li>Install Python using Pyenv , at this time I am using Python 3.11<ul> <li><code>pyenv install 3.11</code> # Install Python 3.11 in computer</li> <li><code>pyenv global 3.11</code> # Set Python 3.11 as global version</li> <li>Check the installation version executing in terminal: <code>python --version</code></li> </ul> </li> <li> <p>Install locally Pipx to Install and Run Python Applications in Isolated Environments</p> <ul> <li> <p>Linux:</p> Install pipx in Linux<pre><code>pip install --user pipx\npython3 -m pipx ensurepath\n</code></pre> <ul> <li>Check the installation version executing in terminal: <code>pipx --version</code></li> </ul> </li> <li> <p>MAC:</p> Install pipx in Mac os<pre><code>brew install pipx\npipx ensurepath\n</code></pre> <ul> <li>Check the installation version executing in terminal: <code>pipx --version</code></li> </ul> </li> </ul> </li> </ol>"},{"location":"local_setup/#general-python-tools","title":"\ud83d\udc0d General Python tools","text":"<p>General Tools that I use to develop Python projects, The most important is Poetry and all this tools are installed using Pipx to have this tools in isolated environments, because applications runs in its own virtual environment to avoid dependencies conflicts and they are available everywhere.</p> <p>When pipx is typically used?</p> <ol> <li>Poetry to manage the dependencies and the virtual environment of the project.<ul> <li><code>pipx install poetry</code></li> </ul> </li> <li>Cruft allows you to maintain all the necessary boilerplate for packaging and building projects separate from the code you intentionally write. Fully compatible with existing Cookiecutter templates.<ul> <li><code>pipx install cruft</code></li> </ul> </li> <li>(optional) Pip-audit to local check the security of the dependencies of the project.<ul> <li><code>pipx install pip-audit</code></li> </ul> </li> <li>(optional) Actionlint to check the syntax of the GitHub Actions configuration files of the project.<ul> <li><code>pipx install actionlint</code></li> </ul> </li> </ol>"},{"location":"local_setup/#start-a-new-data-science-project","title":"\ud83d\udcc1 Start a new data science project","text":"<ol> <li> <p>Start a new project using the Cruft with the Data Science Project Template.</p> create project<pre><code>cruft create https://github.com/JoseRZapata/data-science-project-template\n</code></pre> </li> <li> <p>Answer the questions to create the project.</p> </li> <li> <p>Run <code>make init_env</code> to init Git and Poetry. Or You can do the same running this commands:</p> init environment<pre><code>    git init -b main\n    poetry install\n    poetry run pre-commit install\n    git add .\n    git commit -m \"\ud83c\udf89 Begin a project, Initial commit\"\n    poetry shell\n</code></pre> </li> <li> <p>\ud83c\udf89 Congrats Start coding your project.</p> </li> </ol>"},{"location":"pre-commit/","title":"Pre-commit configuration","text":"<p>this project uses pre-commit to run checks on every commit.</p> <p>Configuration file: .pre-commit-config.yaml</p> <p>If you initialize you project using <code>make init_env</code> pre-commit is already installed and configured. If not, you can install pre-commit running in terminal <code>poetry run pre-commit install</code> in the root of the project.</p>"},{"location":"pre-commit/#pre-commitpre-commit-hooks","title":"pre-commit/pre-commit-hooks","text":"<p>This repository contains some out-of-the-box hooks provided by the pre-commit project.</p> <ul> <li><code>trailing-whitespace</code>: This hook trims trailing whitespace.</li> <li><code>end-of-file-fixer</code>: This hook ensures that a file is either empty, or ends with one newline.</li> <li><code>check-yaml</code>: This hook checks yaml files for parseable syntax.</li> <li><code>check-case-conflict</code>: This hook checks for files with names that would conflict on a case-insensitive filesystem like MacOS HFS+ or Windows FAT.</li> <li><code>debug-statements</code>: This hook checks for Python debug statements.</li> <li><code>detect-private-key</code>: This hook checks for the addition of private keys.</li> <li><code>check-merge-conflict</code>: This hook checks for files that contain merge conflict strings.</li> <li><code>check-ast</code>: This hook checks Python source files for syntactically valid ast.</li> <li><code>check-added-large-files</code>: This hook prevents adding large files. The max size is configurable.</li> </ul>"},{"location":"pre-commit/#astral-shruff-pre-commit","title":"astral-sh/ruff-pre-commit","text":"<p>This repository contains hooks for the Ruff programming language.</p> <ul> <li><code>ruff</code>: This hook runs the Ruff linter with the <code>--fix</code> option to automatically fix issues and a custom configuration file.</li> <li><code>ruff-format</code>: This hook runs the Ruff formatter.</li> </ul>"},{"location":"pre-commit/#pre-commitmirrors-mypy","title":"pre-commit/mirrors-mypy","text":"<p>This repository contains a mirror of mypy for pre-commit.</p> <ul> <li><code>mypy</code>: This hook runs mypy, a static type checker for Python, with a custom configuration file.</li> </ul>"},{"location":"pre-commit/#yelpdetect-secrets","title":"Yelp/detect-secrets","text":"<p>This repository contains a tool to detect secrets in the code base. yelp detect-secrets</p> <ul> <li><code>detect-secrets</code>: This hook runs detect-secrets, a tool to detect secrets in the code base.</li> <li><code>detect-secrets-jupyter</code>: This hook runs detect-secrets specifically for Jupyter notebooks.</li> </ul>"},{"location":"setup_tokens/","title":"\ud83d\udd11 Setup Tokens","text":"<p>Some of the github actions require token keys to be set as secrets in the github repository. The following tokens are required:</p>"},{"location":"setup_tokens/#github-action-secretspat","title":"Github Action secrets.PAT","text":"<p>This is the personal access token for the github repository and It is used to:</p> <ul> <li>Push the MKDocs documentation to the <code>gh-pages</code> branch.</li> <li>Push the pre-commit autoupdate to the <code>main</code> branch.</li> </ul> <p>How to configure the secrets.PAT:</p> <ul> <li>Create in github a Personal Access Token (PAT),for the specific repository. How</li> <li>Give it read/write access to \"Contents\", \"Pull Requests\" and \"Workflows\" under the \"Repository Permissions\" section.</li> <li>Add de PAT to the repository secrets. Go to the repository settings &gt; Secrets and variables &gt; Actions. THen in Repository secrets add a new repository secret and Name it <code>PAT</code> and paste the token.</li> <li>You must explicitly allow GitHub Actions to create pull requests. This setting can be found in a repository's settings under Actions &gt; General &gt; Workflow permissions. select <code>Read repository contents and packages permissions</code></li> </ul>"},{"location":"setup_tokens/#codecov_token","title":"CODECOV_TOKEN","text":"<p>This is the token for codecov. It is used to upload the coverage report to codecov. You can get it from codecov.io. It is not required for local development. https://docs.codecov.com/docs/quick-start</p> <p>You have to add this secret to the github repository. How to add codecov to the github repository: https://docs.codecov.com/docs/adding-the-codecov-token#github-actions</p>"},{"location":"setup_tokens/#references","title":"References","text":"<ul> <li>https://github.com/peter-evans/create-pull-request?tab=readme-ov-file#workflow-permissions</li> <li>https://github.com/peter-evans/create-pull-request/issues/2443</li> <li>https://docs.github.com/en/authentication/keeping-your-account-and-data-secure/managing-your-personal-access-tokens#creating-a-fine-grained-personal-access-token</li> </ul>"},{"location":"vscode/","title":"VSCode configuration settings","text":"<p>The following are some common settings that can be configured in Visual Studio Code to improve the Python development experience.</p> <p>https://github.com/JoseRZapata/data-science-project-template/.vscode/settings.json</p> <pre><code>{\n  \"[python]\": {\n\n    \"editor.codeActionsOnSave\": {\n      \"source.fixAll.ruff\": \"explicit\",\n      \"source.organizeImports.ruff\": \"explicit\",\n    },\n    \"editor.formatOnSave\": true,\n        \"editor.rulers\": [\n      100\n    ]\n  },\n  \"files.exclude\": {\n    \"**/__pycache__\": true\n  },\n  \"python.languageServer\": \"Pylance\",\n  \"editor.formatOnPaste\": true,\n  \"notebook.lineNumbers\": \"on\",\n  \"editor.inlineSuggest.enabled\": true,\n  \"editor.formatOnType\": true,\n  \"git.autofetch\": true,\n  \"editor.defaultFormatter\": \"charliermarsh.ruff\",\n  \"python.terminal.activateEnvInCurrentTerminal\": true,\n}\n</code></pre> <ul> <li> <p><code>[python]</code>: This section applies settings specifically for Python files.</p> </li> <li> <p><code>\"editor.codeActionsOnSave\"</code>: Specifies actions to be performed when a Python file is saved.</p> </li> <li><code>\"source.fixAll.ruff\"</code>: \"explicit\": The Ruff auto-fix feature is set to explicit mode, meaning it will only fix issues when explicitly told to do so.</li> <li><code>\"source.organizeImports.ruff\": \"explicit\"</code>: The Ruff import organization feature is set to explicit mode, meaning it will only organize imports when explicitly told to do so.</li> <li><code>\"editor.formatOnSave\": true</code>: This setting enables automatic code formatting when a Python file is saved.</li> <li><code>\"editor.rulers\": [100]</code>: This setting adds a vertical ruler at the 100th character in the editor for Python files to guide line length.</li> <li> <p><code>\"files.exclude\": {\"**/__pycache__\": true}</code>: This setting hides all pycache directories in the file explorer.</p> </li> <li> <p><code>\"python.languageServer\": \"Pylance\"</code>: This setting specifies Pylance as the language server for Python. A language server provides features like auto-completion and syntax highlighting.</p> </li> <li> <p><code>\"editor.formatOnPaste\": true</code>: This setting enables automatic code formatting when you paste code into the editor.</p> </li> <li> <p><code>\"notebook.lineNumbers\": \"on\"</code>: This setting enables line numbers in Jupyter notebooks.</p> </li> <li> <p><code>\"editor.inlineSuggest.enabled\": true</code>: This setting enables inline suggestions, which show suggested completions as you type.</p> </li> <li> <p><code>\"editor.formatOnType\": true</code>: This setting enables automatic code formatting as you type.</p> </li> <li> <p><code>\"git.autofetch\": true</code>: This setting enables automatic fetching of Git data.</p> </li> <li> <p><code>\"editor.defaultFormatter\": \"charliermarsh.ruff\"</code>: This setting specifies Ruff as the default formatter for code in the editor.</p> </li> <li> <p><code>\"python.terminal.activateEnvInCurrentTerminal\": true</code>: This setting enables automatic activation of the Python environment in the current terminal.</p> </li> <li> <p><code>\"python.defaultInterpreterPath\": \"${workspaceFolder}/.venv\"</code>: This setting specifies the default Python interpreter path to be the virtual environment created in the project.</p> </li> </ul>"},{"location":"github_actions/gh_action_pre-commit-autoupdate/","title":"GitHub Action:Pre-commit auto-update","text":"<p>This GitHub Action file configures a workflow named \"Pre-commit auto-update\". This workflow is responsible for automatically updating the pre-commit hooks in your repository.</p> <p>pre-commit_autoupdate.yml</p>"},{"location":"github_actions/gh_action_pre-commit-autoupdate/#workflow-details","title":"Workflow Details","text":"<ul> <li> <p><code>on</code>: This workflow is triggered in two situations:</p> </li> <li> <p><code>schedule</code>: It runs automatically every Monday at 7:00 UTC (according to the cron schedule <code>\"0 7 * * 1\"</code>).</p> </li> <li><code>workflow_dispatch</code>: This allows the workflow to be manually run from the GitHub user interface.</li> <li><code>jobs</code>: Defines a job named \"pre-commit-update\".</li> </ul>"},{"location":"github_actions/gh_action_pre-commit-autoupdate/#job-pre-commit-update-details","title":"Job \"pre-commit-update\" Details","text":"<p><code>runs-on</code>: This job runs on the latest version of Ubuntu.</p> <p><code>steps</code>: Defines the steps to be followed in this job.</p> <p><code>Checkout</code>: This step uses the actions/checkout@v3 action to get a copy of the repository.</p> <p><code>Update pre-commit hooks</code>: This step uses the <code>brokenpip3/action-pre-commit-update</code> action to update the pre-commit hooks. This action requires a GitHub token to function, which is passed through <code>github-token: ${{ secrets.PAT }}</code>. PAT is a secret stored in the repository settings that contains a personal access token with repository scope.</p> <p>In summary, this workflow takes care of keeping the pre-commit hooks in the repository up-to-date, running automatically every Monday and also allowing manual execution whenever necessary.</p>"}]}