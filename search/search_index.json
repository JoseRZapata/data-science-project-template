{"config":{"lang":["en","es"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Data science project template","text":"<p>A modern template for data science projects with all the necessary tools for experiment, development, testing, and deployment. From notebooks to production.</p> <p>\u2728\ud83d\udcda\u2728 Documentation: https://joserzapata.github.io/data-science-project-template/</p> <p>Source Code: https://github.com/JoseRZapata/data-science-project-template</p>"},{"location":"#features","title":"Features","text":"<ul> <li>Dependency management with UV</li> <li>Virtual environment management with UV</li> <li>Linting with pre-commit and Ruff</li> <li>Continuous integration with GitHub Actions</li> <li>Documentation with mkdocs and mkdocstrings using the mkdocs-materialtheme</li> <li>Automated dependency updates with Dependabot</li> <li>Code formatting with Ruff</li> <li>Import sorting with Ruff using isort rule.</li> <li>Testing with pytest</li> <li>Code coverage with Coverage.py</li> <li>Coverage reporting with Codecov</li> <li>Static type-checking with mypy</li> <li>Security audit with Ruff using bandit rule.</li> <li>Manage project labels with GitHub Labeler</li> </ul> <ul> <li>References</li> </ul>"},{"location":"#creating-a-new-project","title":"\ud83d\udcc1 Creating a New Project","text":""},{"location":"#recommendations","title":"\ud83d\udc4d Recommendations","text":"<p>It is highly recommended to use managers for the python versions, dependencies and virtual environments.</p> <p>This project uses UV, a extremely fast tool to replace pip, pip-tools, Pipx, Poetry, Pyenv, twine, virtualenv, and more.</p> <p>\ud83c\udf1f Check how to setup your environment: https://joserzapata.github.io/data-science-project-template/local_setup/</p>"},{"location":"#via-cruft-recommended","title":"\ud83c\udf6a\ud83e\udd47 Via Cruft - (recommended)","text":"install cruft<pre><code># Install cruft in a isolated environment using uv\n\nuv tool install cruft \n\n# Or Install with pip\n\npip install --user cruft # Install `cruft` on your path for easy access\n</code></pre> create project<pre><code>cruft create https://github.com/JoseRZapata/data-science-project-template\n</code></pre> <p>then inside the project folder, init git and uv environment using Make:</p> install project<pre><code>make init_git\nmake install_env\nsource .venv/bin/activate\n</code></pre>"},{"location":"#via-cookiecutter","title":"\ud83c\udf6a Via Cookiecutter","text":"install cookiecutter<pre><code>uv tool install cookiecutter # Install cruft in a isolated environment\n\n# Or Install with pip\n\npip install --user cookiecutter # Install `cookiecutter` on your path for easy access\n</code></pre> create project<pre><code>cookiecutter gh:JoseRZapata/data-science-project-template\n</code></pre> <p>Note: Cookiecutter uses <code>gh:</code> as short-hand for <code>https://github.com/</code></p>"},{"location":"#linking-an-existing-project","title":"\ud83d\udd17  Linking an Existing Project","text":"<p>If the project was originally installed via Cookiecutter, you must first use Cruft to link the project with the original template:</p> <pre><code>cruft link https://github.com/JoseRZapata/data-science-project-template\n</code></pre> <p>Then/else:</p> <pre><code>cruft update\n</code></pre>"},{"location":"#project-structure","title":"\ud83d\uddc3\ufe0f Project structure","text":"<p>Folder structure for data science projects  why?</p> <ul> <li>Data structure</li> <li>Pipelines based on Feature/Training/Inference Pipelines</li> </ul> <pre><code>.\n\u251c\u2500\u2500 .code_quality\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 mypy.ini                        # mypy configuration\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 ruff.toml                       # ruff configuration\n\u251c\u2500\u2500 .github                             # github configuration\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 actions\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 python-poetry-env\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0     \u2514\u2500\u2500 action.yml              # github action to setup python environment\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 dependabot.md                   # github action to update dependencies\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 pull_request_template.md        # template for pull requests\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 workflows                       # github actions workflows\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 ci.yml                      # run continuous integration (tests, pre-commit, etc.)\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 dependency_review.yml       # review dependencies\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 docs.yml                    # build documentation (mkdocs)\n\u2502\u00a0\u00a0     \u2514\u2500\u2500 pre-commit_autoupdate.yml   # update pre-commit hooks\n\u251c\u2500\u2500 .vscode                             # vscode configuration\n|   \u251c\u2500\u2500 extensions.json                 # list of recommended extensions\n|   \u251c\u2500\u2500 launch.json                     # vscode launch configuration\n|   \u2514\u2500\u2500 settings.json                   # vscode settings\n\u251c\u2500\u2500 conf                                # folder configuration files\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 config.yaml                     # main configuration file\n\u251c\u2500\u2500 data\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 01_raw                          # raw immutable data\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 02_intermediate                 # typed data\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 03_primary                      # domain model data\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 04_feature                      # model features\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 05_model_input                  # often called 'master tables'\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 06_models                       # serialized models\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 07_model_output                 # data generated by model runs\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 08_reporting                    # reports, results, etc\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 README.md                       # description of the data structure\n\u251c\u2500\u2500 docs                                # documentation for your project\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 index.md                        # documentation homepage\n\u251c\u2500\u2500 models                              # store final models\n\u251c\u2500\u2500 notebooks\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 1-data                          # data extraction and cleaning\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 2-exploration                   # exploratory data analysis (EDA)\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 3-analysis                      # Statistical analysis, hypothesis testing.\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 4-feat_eng                      # feature engineering (creation, selection, and transformation.)\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 5-models                        # model training, evaluation and hyperparameter tuning.\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 6-interpretation                # model interpretation\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 7-deploy                        # model packaging, deployment strategies.\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 8-reports                       # story telling, summaries and analysis conclusions.\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 notebook_template.ipynb         # template for notebooks\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 README.md                       # information about the notebooks\n\u251c\u2500\u2500 src                                 # source code for use in this project\n\u2502   \u251c\u2500\u2500 README.md                       # description of src structure\n\u2502   \u251c\u2500\u2500 tmp_mock.py                     # example python file\n\u2502   \u251c\u2500\u2500 data                            # data extraction, validation, processing, transformation\n\u2502   \u251c\u2500\u2500 model                           # model training, evaluation, validation, export\n\u2502   \u251c\u2500\u2500 inference                       # model prediction, serving, monitoring\n\u2502   \u2514\u2500\u2500 pipelines                       # orchestration of pipelines\n\u2502       \u251c\u2500\u2500 feature_pipeline            # transforms raw data into features and labels\n\u2502       \u251c\u2500\u2500 training_pipeline           # transforms features and labels into a model\n\u2502       \u2514\u2500\u2500 inference_pipeline          # takes features and a trained model for predictions\n\u251c\u2500\u2500 tests                               # test code for your project\n\u2502   \u251c\u2500\u2500 test_mock.py                    # example test file\n\u2502   \u251c\u2500\u2500 data                            # tests for data module\n\u2502   \u251c\u2500\u2500 model                           # tests for model module\n\u2502   \u251c\u2500\u2500 inference                       # tests for inference module\n\u2502   \u2514\u2500\u2500 pipelines                       # tests for pipelines module\n\u251c\u2500\u2500 .editorconfig                       # editor configuration\n\u251c\u2500\u2500 .gitignore                          # files to ignore in git\n\u251c\u2500\u2500 .pre-commit-config.yaml             # configuration for pre-commit hooks\n\u251c\u2500\u2500 codecov.yml                         # configuration for codecov\n\u251c\u2500\u2500 Makefile                            # useful commands to setup environment, run tests, etc.\n\u251c\u2500\u2500 mkdocs.yml                          # configuration for mkdocs documentation\n\u251c\u2500\u2500 pyproject.toml                      # dependencies and configuration project file\n\u251c\u2500\u2500 uv.lock                             # locked dependencies\n\u2514\u2500\u2500 README.md                           # description of your project    \n</code></pre>"},{"location":"#features-and-tools","title":"\u2728 Features and Tools","text":""},{"location":"#project-standardization-and-automation","title":"\ud83d\ude80 Project Standardization and Automation","text":""},{"location":"#developer-workflow-automation","title":"\ud83d\udd28 Developer Workflow Automation","text":"<ul> <li>Python packaging, dependency management and environment management   with UV - <code>why use a management, (uv is a replacement for poetry)</code></li> <li>Project workflow orchestration   with Make as an interface shim<ul> <li>Self-documenting Makefile; just type   <code>make</code> on the command line to display auto-generated documentation on available   targets:</li> </ul> </li> <li>Automated Cookiecutter template synchronization with Cruft - <code>why?</code></li> <li>Code quality tooling automation and management with pre-commit</li> <li>Continuous integration and deployment with GitHub Actions</li> <li>Project configuration files  with Hydra - <code>why?</code></li> </ul>"},{"location":"#conditionally-rendered-python-package-or-project-boilerplate","title":"\ud83c\udf31 Conditionally Rendered Python Package or Project Boilerplate","text":"<ul> <li>Optional: Jupyter support</li> </ul>"},{"location":"#maintainability","title":"\ud83d\udd27 Maintainability","text":""},{"location":"#type-checking-and-data-validation","title":"\ud83c\udff7\ufe0f  Type Checking and Data Validation","text":"<ul> <li>Static type-checking with Mypy</li> </ul>"},{"location":"#testingcoverage","title":"\u2705 \ud83e\uddea Testing/Coverage","text":"<ul> <li>Testing with Pytest</li> <li>Code coverage with Coverage.py</li> <li>Coverage reporting with Codecov</li> </ul>"},{"location":"#linting","title":"\ud83d\udea8 Linting","text":""},{"location":"#code-quality","title":"\ud83d\udd0d Code quality","text":"<ul> <li>Ruff An extremely fast (10x-100x faster) Python linter and code formatter, written in Rust.<ul> <li>Replacement for ~~Pylint~~, ~~Flake8~~ (including major plugins) and more linters under a single, common interface</li> </ul> </li> <li>ShellCheck</li> <li>Unsanitary commits:<ul> <li>Secrets with <code>detect-secrets</code></li> <li>Large files with <code>check-added-large-files</code></li> <li>Files that contain merge conflict strings.check-merge-conflict</li> </ul> </li> </ul>"},{"location":"#code-formatting","title":"\ud83c\udfa8 Code formatting","text":"<ul> <li> <p>Ruff An extremely fast (10x-100x faster) Python linter and code formatter, written in Rust.</p> <ul> <li>Replacement for ~~Black~~, ~~isort~~, ~~pyupgrade~~ and more formatters under a single, common interface</li> </ul> </li> <li> <p>General file formatting:</p> <ul> <li><code>end-of-file-fixer</code></li> <li><code>pretty-format-json</code></li> <li>(trim) <code>trailing-whitespace</code></li> <li><code>check-yaml</code></li> </ul> </li> </ul>"},{"location":"#cicd","title":"\ud83d\udc77 CI/CD","text":""},{"location":"#automatic-dependency-updates","title":"Automatic Dependency updates","text":"<ul> <li> <p>Dependency updates with Dependabot, Automated Dependabot PR merging with the Dependabot Auto Merge GitHub Action</p> </li> <li> <p>This is a replacement for pip-audit , In your local environment, If you want to check for vulnerabilities in your dependencies you can use [pip-audit].</p> </li> </ul>"},{"location":"#dependency-review-in-pr","title":"Dependency Review in PR","text":"<ul> <li>Dependency Review  with dependency-review-action, This action scans your pull requests for dependency changes, and will raise an error if any vulnerabilities or invalid licenses are being introduced.</li> </ul>"},{"location":"#pre-commit-automatic-updates","title":"Pre-commit automatic updates","text":"<ul> <li>Automatic updates with GitHub Actions workflow <code>.github/workflows/pre-commit_autoupdate.yml</code></li> </ul>"},{"location":"#security","title":"\ud83d\udd12 Security","text":""},{"location":"#static-application-security-testing-sast","title":"\ud83d\udd0f Static Application Security Testing (SAST)","text":"<ul> <li>Code vulnerabilities with Bandit using Ruff</li> </ul>"},{"location":"#accessibility","title":"\u2328\ufe0f Accessibility","text":""},{"location":"#automation-tool-makefile","title":"\ud83d\udd28 Automation tool (Makefile)","text":"<p>Makefile to automate the setup of your environment, the installation of dependencies, the execution of tests, etc. in terminal type <code>make</code> to see the available commands</p> <pre><code>Target                Description\n-------------------   ----------------------------------------------------\ncheck                 Run code quality tools with pre-commit hooks.\ndocs_test             Test if documentation can be built without warnings or errors\ndocs_view             Build and serve the documentation\ninit_env              Install dependencies with uv and activate env\ninit_git              Initialize git repository\ninstall_data_libs     Install pandas, scikit-learn, Jupyter, seaborn\npre-commit_update     Update pre-commit hooks\ntest                  Test the code with pytest and coverage\n</code></pre>"},{"location":"#project-documentation","title":"\ud83d\udcdd Project Documentation","text":"<ul> <li>Documentation building   with MkDocs - Tutorial<ul> <li>Powered by mkdocs-material</li> <li>Rich automatic documentation from type annotations and docstrings (NumPy, Google, etc.) with mkdocstrings</li> </ul> </li> </ul>"},{"location":"#templates","title":"\ud83d\uddc3\ufe0f Templates","text":"<ul> <li>Pull Request template</li> <li>Notebook template</li> </ul>"},{"location":"#good-practices","title":"Good practices","text":"<ul> <li>https://www.conventionalcommits.org/</li> <li>https://keepachangelog.com/</li> </ul>"},{"location":"#references","title":"References","text":"<ul> <li>https://drivendata.github.io/cookiecutter-data-science/</li> <li>https://github.com/crmne/cookiecutter-modern-datascience</li> <li>https://github.com/fpgmaas/cookiecutter-poetry</li> <li>https://github.com/khuyentran1401/data-science-template</li> <li>https://github.com/woltapp/wolt-python-package-cookiecutter</li> <li>https://khuyentran1401.github.io/reproducible-data-science/structure_project/introduction.html</li> <li>https://github.com/TeoZosa/cookiecutter-cruft-poetry-tox-pre-commit-ci-cd</li> <li>https://github.com/cjolowicz/cookiecutter-hypermodern-python</li> <li>https://github.com/gotofritz/cookiecutter-gotofritz-poetry</li> <li>https://github.com/kedro-org/kedro-starters</li> </ul>"},{"location":"data_schema/","title":"Data structure","text":"<p>layered data-engineering convention</p> <p></p> <code>Folder in data</code> <code>Description</code> <code>raw</code> initial start of the pipeline, containing the sourced data model(s) that should never be changed, it forms your single source of truth to work from. these data models are typically un-typed in most cases e.g. csv, but this will vary from case to case <code>intermediate</code> optional data model(s), which are introduced to type your raw data model(s), e.g. converting string based values into their current typed representation <code>primary</code> domain specific data model(s) containing cleansed, transformed and wrangled data from either raw or intermediate, which forms your layer that you input into your feature engineering <code>feature</code> analytics specific data model(s) containing a set of features defined against the primary data, which are grouped by feature area of analysis and stored against a common dimension <code>model input</code> analytics specific data model(s) containing all feature data against a common dimension and in the case of live projects against an analytics run date to ensure that you track the historical changes of the features over time <code>models</code> stored, serialised pre-trained machine learning models <code>model output</code> analytics specific data model(s) containing the results generated by the model based on the model input data <code>reporting</code> reporting data model(s) that are used to combine a set of primary, feature, model input and model output data used to drive the dashboard and the views constructed. it encapsulates and removes the need to define any blending or joining of data, improve performance and replacement of presentation layer without having to redefine the data models"},{"location":"data_schema/#references","title":"References","text":"<ul> <li>https://docs.kedro.org/en/stable/faq/faq.html#what-is-data-engineering-convention</li> <li>https://towardsdatascience.com/the-importance-of-layered-thinking-in-data-engineering-a09f685edc71</li> </ul>"},{"location":"directory_hierarchy/","title":"\ud83d\uddc2\ufe0f Directory Hierarchy","text":"<p>Folder structure for data science projects  why?</p> <ul> <li>Data structure</li> <li>Pipelines based on Feature/Training/Inference Pipelines</li> </ul> <pre><code>.\n\u251c\u2500\u2500 .code_quality\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 mypy.ini                        # mypy configuration\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 ruff.toml                       # ruff configuration\n\u251c\u2500\u2500 .github                             # github configuration\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 actions\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 python-poetry-env\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0     \u2514\u2500\u2500 action.yml              # github action to setup python environment\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 dependabot.md                   # github action to update dependencies\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 pull_request_template.md        # template for pull requests\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 workflows                       # github actions workflows\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 ci.yml                      # run continuous integration (tests, pre-commit, etc.)\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 dependency_review.yml       # review dependencies\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 docs.yml                    # build documentation (mkdocs)\n\u2502\u00a0\u00a0     \u2514\u2500\u2500 pre-commit_autoupdate.yml   # update pre-commit hooks\n\u251c\u2500\u2500 .vscode                             # vscode configuration\n|   \u251c\u2500\u2500 extensions.json                 # list of recommended extensions\n|   \u251c\u2500\u2500 launch.json                     # vscode launch configuration\n|   \u2514\u2500\u2500 settings.json                   # vscode settings\n\u251c\u2500\u2500 conf                                # folder configuration files\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 config.yaml                     # main configuration file\n\u251c\u2500\u2500 data\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 01_raw                          # raw immutable data\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 02_intermediate                 # typed data\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 03_primary                      # domain model data\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 04_feature                      # model features\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 05_model_input                  # often called 'master tables'\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 06_models                       # serialized models\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 07_model_output                 # data generated by model runs\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 08_reporting                    # reports, results, etc\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 README.md                       # description of the data structure\n\u251c\u2500\u2500 docs                                # documentation for your project\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 index.md                        # documentation homepage\n\u251c\u2500\u2500 models                              # store final models\n\u251c\u2500\u2500 notebooks\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 1-data                          # data extraction and cleaning\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 2-exploration                   # exploratory data analysis (EDA)\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 3-analysis                      # Statistical analysis, hypothesis testing.\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 4-feat_eng                      # feature engineering (creation, selection, and transformation.)\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 5-models                        # model training, evaluation and hyperparameter tuning.\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 6-interpretation                # model interpretation\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 7-deploy                        # model packaging, deployment strategies.\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 8-reports                       # story telling, summaries and analysis conclusions.\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 notebook_template.ipynb         # template for notebooks\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 README.md                       # information about the notebooks\n\u251c\u2500\u2500 src                                 # source code for use in this project\n\u2502   \u251c\u2500\u2500 README.md                       # description of src structure\n\u2502   \u251c\u2500\u2500 tmp_mock.py                     # example python file\n\u2502   \u251c\u2500\u2500 data                            # data extraction, validation, processing, transformation\n\u2502   \u251c\u2500\u2500 model                           # model training, evaluation, validation, export\n\u2502   \u251c\u2500\u2500 inference                       # model prediction, serving, monitoring\n\u2502   \u2514\u2500\u2500 pipelines                       # orchestration of pipelines\n\u2502       \u251c\u2500\u2500 feature_pipeline            # transforms raw data into features and labels\n\u2502       \u251c\u2500\u2500 training_pipeline           # transforms features and labels into a model\n\u2502       \u2514\u2500\u2500 inference_pipeline          # takes features and a trained model for predictions\n\u251c\u2500\u2500 tests                               # test code for your project\n\u2502   \u251c\u2500\u2500 test_mock.py                    # example test file\n\u2502   \u251c\u2500\u2500 data                            # tests for data module\n\u2502   \u251c\u2500\u2500 model                           # tests for model module\n\u2502   \u251c\u2500\u2500 inference                       # tests for inference module\n\u2502   \u2514\u2500\u2500 pipelines                       # tests for pipelines module\n\u251c\u2500\u2500 .editorconfig                       # editor configuration\n\u251c\u2500\u2500 .gitignore                          # files to ignore in git\n\u251c\u2500\u2500 .pre-commit-config.yaml             # configuration for pre-commit hooks\n\u251c\u2500\u2500 codecov.yml                         # configuration for codecov\n\u251c\u2500\u2500 Makefile                            # useful commands to setup environment, run tests, etc.\n\u251c\u2500\u2500 mkdocs.yml                          # configuration for mkdocs documentation\n\u251c\u2500\u2500 pyproject.toml                      # dependencies and configuration project file\n\u251c\u2500\u2500 uv.lock                             # locked dependencies\n\u2514\u2500\u2500 README.md                           # description of your project    \n</code></pre>"},{"location":"directory_hierarchy/#features-and-tools","title":"\u2728 Features and Tools","text":""},{"location":"local_setup/","title":"\ud83d\udee0\ufe0f Local Dev environment setup","text":"<p>I develop data science python projects in Linux OS or MAC OS. (For Windows OS I recommend WSL and run commands as Linux OS).</p> <p>I setup my local development environment using the following steps:</p>"},{"location":"local_setup/#computer-setup-to-develop-with-python","title":"\ud83d\udcbb Computer setup to develop with Python","text":"<ol> <li>Install Git<ul> <li>Linux: <code>sudo apt-get install git</code></li> <li>MAC: <code>brew install git</code></li> </ul> </li> <li>Install Make<ul> <li>Linux: <code>sudo apt-get install make</code></li> <li>MAC: <code>brew install make</code></li> </ul> </li> <li> <p>Install locally UV to Manage python versions, dependencies and virtual environments.</p> <ul> <li> <p>Linux:</p> Install uv in Linux or MACOS<pre><code>curl -LsSf https://astral.sh/uv/install.sh | sh\n</code></pre> <ul> <li>Check the installation version executing in terminal: <code>uv version</code></li> </ul> </li> </ul> </li> <li> <p>Install Python using UV, at this time I am using Python 3.11  </p> <ul> <li><code>uv python install 3.11</code> # Install Python 3.11 in computer</li> </ul> </li> <li> <p>In the folder of the project, set the Python version and create a virtual environment.</p> </li> <li>If the project has <code>uv.lock</code> file<ul> <li><code>uv sync</code> # Create virtual Environment and install dependencies.</li> <li><code>source .venv/bin/activate</code> # Activate Environment    2.</li> <li><code>uv venv --python 3.11</code> # Create a Python 3.11 virtual environment</li> <li><code>source .venv/bin/activate</code> # Activate the Python 3.11 virtual environment</li> <li>Check the installation version executing in terminal: <code>python --version</code></li> </ul> </li> </ol> <p>Note: To deactivate the virtual enviroment in terminal type: <code>deactivate</code></p>"},{"location":"local_setup/#general-python-tools","title":"\ud83d\udc0d General Python tools","text":"<p>General Tools that I use to develop Python projects, The most important is UV to manage python versions, virtual environments, manage dependencies for the projects and to have tools in isolated environments, because applications runs in its own virtual environment to avoid dependencies conflicts and they are available everywhere.</p> <p>UV Highligths</p> <ol> <li>Cruft allows you to maintain all the necessary boilerplate for packaging and building projects separate from the code you intentionally write. Fully compatible with existing Cookiecutter templates.<ul> <li><code>uv tool install cruft</code></li> </ul> </li> <li>(optional) Pip-audit to local check the security of the dependencies of the project.<ul> <li><code>uv tool install pip-audit</code></li> </ul> </li> <li>(optional) Actionlint to check the syntax of the GitHub Actions configuration files of the project.<ul> <li><code>uv tool install actionlint</code></li> </ul> </li> </ol> <p>Note: UV replace tool like Pyenv, Poetry and other ones to manage the python versions, environments and dependencies.</p>"},{"location":"local_setup/#start-a-new-data-science-project","title":"\ud83d\udcc1 Start a new data science project","text":"<ol> <li> <p>Start a new project using the Cruft with the Data Science Project Template.</p> create project<pre><code>cruft create https://github.com/JoseRZapata/data-science-project-template\n</code></pre> </li> <li> <p>Answer the questions to create the project.</p> </li> <li> <p>Run <code>make init_git</code> to init Git Or You can do the same running this commands:</p> init environment<pre><code>    git init -b main\n    git add .\n    git commit -m \"\ud83c\udf89 Begin a project, Initial commit\"\n</code></pre> </li> <li> <p>Run <code>make install_env</code> to install the dependencies of the project. Or You can do the same running this commands:</p> install dependencies<pre><code>    uv sync --all-groups\n    uv run pre-commit install\n</code></pre> </li> <li> <p>\ud83c\udf89 Congrats Start coding your project.</p> </li> <li> <p>If you want to push this repository to GitHub, first create a new repository in GitHub and then run the following commands to push your local repository to GitHub.</p> push to GitHub<pre><code>    git remote add origin &lt;your-repo-url&gt;\n    git branch -M main\n    git push -u origin main\n</code></pre> </li> </ol>"},{"location":"pre-commit/","title":"Pre-commit configuration","text":"<p>this project uses pre-commit to run checks on every commit.</p> <p>Configuration file: .pre-commit-config.yaml</p> <p>If you initialize you project using <code>make install</code> pre-commit is already installed and configured. If not, you can install pre-commit running in terminal <code>uv run pre-commit install</code> in the root of the project.</p>"},{"location":"pre-commit/#pre-commitpre-commit-hooks","title":"pre-commit/pre-commit-hooks","text":"<p>This repository contains some out-of-the-box hooks provided by the pre-commit project.</p> <ul> <li><code>trailing-whitespace</code>: This hook trims trailing whitespace.</li> <li><code>end-of-file-fixer</code>: This hook ensures that a file is either empty, or ends with one newline.</li> <li><code>check-yaml</code>: This hook checks yaml files for parseable syntax.</li> <li><code>check-case-conflict</code>: This hook checks for files with names that would conflict on a case-insensitive filesystem like MacOS HFS+ or Windows FAT.</li> <li><code>debug-statements</code>: This hook checks for Python debug statements.</li> <li><code>detect-private-key</code>: This hook checks for the addition of private keys.</li> <li><code>check-merge-conflict</code>: This hook checks for files that contain merge conflict strings.</li> <li><code>check-ast</code>: This hook checks Python source files for syntactically valid ast.</li> <li><code>check-added-large-files</code>: This hook prevents adding large files. The max size is configurable.</li> </ul>"},{"location":"pre-commit/#astral-shruff-pre-commit","title":"astral-sh/ruff-pre-commit","text":"<p>This repository contains hooks for the Ruff programming language.</p> <ul> <li><code>ruff</code>: This hook runs the Ruff linter with the <code>--fix</code> option to automatically fix issues in scripts and notebooks and a custom configuration file.</li> <li><code>ruff-format</code>: This hook runs the Ruff formatter.</li> </ul>"},{"location":"pre-commit/#pre-commitmirrors-mypy","title":"pre-commit/mirrors-mypy","text":"<p>This repository contains a mirror of mypy for pre-commit.</p> <ul> <li><code>mypy</code>: This hook runs mypy, a static type checker for Python, with a custom configuration file.</li> </ul>"},{"location":"pre-commit/#commitizen-toolscommitizen","title":"commitizen-tools/commitizen","text":"<p>This repository contains hooks for commitizen.</p> <ul> <li><code>commitizen</code>: This hook checks that commit messages follow the conventional commit format.</li> </ul>"},{"location":"setup_tokens/","title":"\ud83d\udd11 Setup Tokens","text":"<p>Some of the github actions require token keys to be set as secrets in the github repository. The following tokens are required:</p>"},{"location":"setup_tokens/#github-action-secretspat","title":"Github Action secrets.PAT","text":"<p>This is the personal access token for the github repository and It is used to:</p> <ul> <li>Push the MKDocs documentation to the <code>gh-pages</code> branch.</li> <li>Push the pre-commit autoupdate to the <code>main</code> branch.</li> </ul> <p>How to configure the secrets.PAT:</p> <ul> <li>Create in github a Personal Access Token (PAT),for the specific repository. How</li> <li>Give it read/write access to \"Contents\", \"Pull Requests\" and \"Workflows\" under the \"Repository Permissions\" section.</li> <li>Add de PAT to the repository secrets. Go to the repository settings &gt; Secrets and variables &gt; Actions. THen in Repository secrets add a new repository secret and Name it <code>PAT</code> and paste the token.</li> <li>You must explicitly allow GitHub Actions to create pull requests. This setting can be found in a repository's settings under Actions &gt; General &gt; Workflow permissions. select <code>Read repository contents and packages permissions</code></li> </ul>"},{"location":"setup_tokens/#codecov_token","title":"CODECOV_TOKEN","text":"<p>This is the token for codecov. It is used to upload the coverage report to codecov. You can get it from codecov.io. It is not required for local development. https://docs.codecov.com/docs/quick-start</p> <p>You have to add this secret to the github repository. How to add codecov to the github repository: https://docs.codecov.com/docs/adding-the-codecov-token#github-actions</p>"},{"location":"setup_tokens/#references","title":"References","text":"<ul> <li>https://github.com/peter-evans/create-pull-request?tab=readme-ov-file#workflow-permissions</li> <li>https://github.com/peter-evans/create-pull-request/issues/2443</li> <li>https://docs.github.com/en/authentication/keeping-your-account-and-data-secure/managing-your-personal-access-tokens#creating-a-fine-grained-personal-access-token</li> </ul>"},{"location":"vscode/","title":"VSCode configuration settings","text":"<p>The following are some common settings that can be configured in Visual Studio Code to improve the Python development experience.</p> <p>https://github.com/JoseRZapata/data-science-project-template/.vscode/settings.json</p> <pre><code>{\n  \"[python]\": {\n\n    \"editor.codeActionsOnSave\": {\n      \"source.fixAll.ruff\": \"explicit\",\n      \"source.organizeImports.ruff\": \"explicit\",\n    },\n    \"editor.formatOnSave\": true,\n        \"editor.rulers\": [\n      100\n    ]\n  },\n  \"files.exclude\": {\n    \"**/__pycache__\": true\n  },\n  \"python.languageServer\": \"Pylance\",\n  \"editor.formatOnPaste\": true,\n  \"notebook.lineNumbers\": \"on\",\n  \"editor.inlineSuggest.enabled\": true,\n  \"editor.formatOnType\": true,\n  \"git.autofetch\": true,\n  \"editor.defaultFormatter\": \"charliermarsh.ruff\",\n  \"python.terminal.activateEnvInCurrentTerminal\": true,\n}\n</code></pre> <ul> <li> <p><code>[python]</code>: This section applies settings specifically for Python files.</p> </li> <li> <p><code>\"editor.codeActionsOnSave\"</code>: Specifies actions to be performed when a Python file is saved.</p> </li> <li><code>\"source.fixAll.ruff\"</code>: \"explicit\": The Ruff auto-fix feature is set to explicit mode, meaning it will only fix issues when explicitly told to do so.</li> <li><code>\"source.organizeImports.ruff\": \"explicit\"</code>: The Ruff import organization feature is set to explicit mode, meaning it will only organize imports when explicitly told to do so.</li> <li><code>\"editor.formatOnSave\": true</code>: This setting enables automatic code formatting when a Python file is saved.</li> <li><code>\"editor.rulers\": [100]</code>: This setting adds a vertical ruler at the 100th character in the editor for Python files to guide line length.</li> <li> <p><code>\"files.exclude\": {\"**/__pycache__\": true}</code>: This setting hides all pycache directories in the file explorer.</p> </li> <li> <p><code>\"python.languageServer\": \"Pylance\"</code>: This setting specifies Pylance as the language server for Python. A language server provides features like auto-completion and syntax highlighting.</p> </li> <li> <p><code>\"editor.formatOnPaste\": true</code>: This setting enables automatic code formatting when you paste code into the editor.</p> </li> <li> <p><code>\"notebook.lineNumbers\": \"on\"</code>: This setting enables line numbers in Jupyter notebooks.</p> </li> <li> <p><code>\"editor.inlineSuggest.enabled\": true</code>: This setting enables inline suggestions, which show suggested completions as you type.</p> </li> <li> <p><code>\"editor.formatOnType\": true</code>: This setting enables automatic code formatting as you type.</p> </li> <li> <p><code>\"git.autofetch\": true</code>: This setting enables automatic fetching of Git data.</p> </li> <li> <p><code>\"editor.defaultFormatter\": \"charliermarsh.ruff\"</code>: This setting specifies Ruff as the default formatter for code in the editor.</p> </li> <li> <p><code>\"python.terminal.activateEnvInCurrentTerminal\": true</code>: This setting enables automatic activation of the Python environment in the current terminal.</p> </li> <li> <p><code>\"python.defaultInterpreterPath\": \"${workspaceFolder}/.venv\"</code>: This setting specifies the default Python interpreter path to be the virtual environment created in the project.</p> </li> </ul>"},{"location":"github_actions/automerge/","title":"GitHub Action: Automerge","text":"<p>This GitHub Action workflow automates the merging of pull requests labeled with <code>automerge</code>. It supports scheduled runs and manual execution, ensuring streamlined integration of changes that meet predefined criteria.</p> <p>automerge.yml</p>"},{"location":"github_actions/automerge/#workflow-details","title":"Workflow Details","text":""},{"location":"github_actions/automerge/#triggers-on","title":"Triggers (<code>on</code>)","text":"<ul> <li><code>schedule</code>:<ul> <li>Automatically runs daily at 08:00 UTC (according to the cron schedule: <code>0 8 * * *</code>).</li> </ul> </li> <li><code>workflow_dispatch</code>:<ul> <li>Allows manual execution of the workflow via the GitHub Actions interface.</li> </ul> </li> </ul>"},{"location":"github_actions/automerge/#job-details","title":"Job Details","text":""},{"location":"github_actions/automerge/#automerge","title":"automerge","text":"<p><code>runs-on</code>: <code>ubuntu-latest</code></p> <p>Steps:</p> <ol> <li>Automerge:</li> <li>Uses the <code>pascalgn/automerge-action@v0.16.3</code> action to merge pull requests that meet specific criteria.</li> <li>Environment Variables:<ul> <li><code>GITHUB_TOKEN</code>: A personal access token (PAT) stored in the repository's secrets for authentication.</li> <li><code>MERGE_METHOD</code>: Specifies the merge method. In this case, the <code>squash</code> method is used, which combines all commits into a single commit when merging.</li> <li><code>MERGE_REQUIRED_APPROVALS</code>: Defines the required number of approvals for merging. Here, it is set to <code>\"0\"</code>, meaning no approvals are required.</li> <li><code>MERGE_LABELS</code>: Specifies the label (<code>automerge</code>) that triggers the automerge process.</li> </ul> </li> </ol> <p>In summary, this workflow simplifies the process of merging pull requests by automatically merging those labeled with <code>automerge</code> using the <code>squash</code> method. It can be executed daily or manually, ensuring flexibility and efficiency in the development process.</p>"},{"location":"github_actions/ci/","title":"GitHub Action: CI/CD Tests","text":"<p>This GitHub Action workflow is designed to streamline and automate the CI/CD processes for your project. The workflow is triggered on pull requests and pushes to the <code>main</code> branch. It performs several key tasks including linting, pre-commit checks, project update validation, and running tests with coverage reporting.</p> <p>ci.yml</p>"},{"location":"github_actions/ci/#workflow-details","title":"Workflow Details","text":""},{"location":"github_actions/ci/#triggers-on","title":"Triggers (<code>on</code>)","text":"<ul> <li><code>pull_request</code>: The workflow runs when a pull request is opened or updated.</li> <li><code>push</code>: The workflow is triggered for pushes to the <code>main</code> branch.</li> </ul>"},{"location":"github_actions/ci/#jobs-overview","title":"Jobs Overview","text":"<p>The workflow defines four jobs:</p> <ol> <li><code>actionlint</code>: Validates the syntax and structure of GitHub Action workflow files.</li> <li><code>lint-cruft</code>: Ensures no <code>.rej</code> files are present, verifying that project updates were applied correctly.</li> <li><code>pre-commit</code>: Executes pre-commit hooks to enforce code standards and formatting in all files.</li> <li><code>test</code>: Runs unit tests with coverage reporting and uploads the results to Codecov.</li> </ol>"},{"location":"github_actions/ci/#job-details","title":"Job Details","text":""},{"location":"github_actions/ci/#1-actionlint","title":"1. actionlint","text":"<p><code>runs-on</code>: <code>ubuntu-latest</code></p> <p>Steps:</p> <ul> <li>Checkout: Uses <code>actions/checkout@v4</code> to fetch the repository.</li> <li>Download actionlint: Fetches <code>actionlint</code>  using a bash script.</li> <li>Check workflow files: Runs <code>actionlint</code> to validate the workflow files.</li> </ul>"},{"location":"github_actions/ci/#2-lint-cruft","title":"2. lint-cruft","text":"<p><code>runs-on</code>: <code>ubuntu-latest</code></p> <p>Steps:</p> <ul> <li>Checkout: Uses <code>actions/checkout@v4</code> to fetch the repository.</li> <li>Check for <code>.rej</code> files: Fails the job if <code>.rej</code> files are found, indicating an unsuccessful project structure update.</li> </ul>"},{"location":"github_actions/ci/#3-pre-commit","title":"3. pre-commit","text":"<p><code>runs-on</code>: <code>ubuntu-latest</code></p> <p>Steps:</p> <ul> <li>Checkout: Uses <code>actions/checkout@v4</code> to fetch the repository.</li> <li>Install <code>uv</code>: Sets up the <code>uv</code> tool using <code>astral-sh/setup-uv@v6</code>.</li> <li>Run pre-commit hooks: Executes all pre-commit hooks across the repository, displaying any failures with colorized diffs.</li> </ul>"},{"location":"github_actions/ci/#4-test","title":"4. test","text":"<p><code>runs-on</code>: <code>ubuntu-latest</code></p> <p>Steps:</p> <ul> <li>Checkout: Uses <code>actions/checkout@v4</code> to fetch the repository.</li> <li>Install <code>uv</code>: Sets up the <code>uv</code> tool using <code>astral-sh/setup-uv@v6</code>.</li> <li>Set up Python: Configures Python according to the <code>.python-version</code> file using <code>actions/setup-python@v5</code>.</li> <li>Install the project: Installs the project with all extras and development dependencies using <code>uv sync</code>.</li> <li>Run tests with coverage: Executes tests with <code>pytest</code> and generates a coverage report in XML format.</li> <li>Upload coverage report: Uploads the coverage report to Codecov using <code>codecov/codecov-action@v5</code>. Requires a <code>CODECOV_TOKEN</code> secret stored in the repository settings.</li> </ul> <p>In summary, this workflow automates linting, project update validation, pre-commit checks, testing, and coverage reporting for a robust CI/CD pipeline.</p>"},{"location":"github_actions/dependency_review/","title":"GitHub Action: Dependency Review","text":"<p>This GitHub Action workflow is designed to automatically review changes to your project dependencies during pull requests. It uses GitHub's <code>dependency-review-action</code> to analyze the dependency graph and flag potential security issues or unwanted changes in dependencies.</p> <p>dependency-review.yml</p>"},{"location":"github_actions/dependency_review/#workflow-details","title":"Workflow Details","text":""},{"location":"github_actions/dependency_review/#triggers-on","title":"Triggers (<code>on</code>)","text":"<ul> <li><code>pull_request</code>: The workflow is triggered whenever a pull request is created or updated, ensuring dependency reviews are performed as part of the code review process.</li> </ul>"},{"location":"github_actions/dependency_review/#permissions","title":"Permissions","text":"<ul> <li><code>contents: read</code>: Grants the workflow read-only access to the repository's contents, which is sufficient for performing the dependency review.</li> </ul>"},{"location":"github_actions/dependency_review/#job-details","title":"Job Details","text":""},{"location":"github_actions/dependency_review/#dependency-review","title":"dependency-review","text":"<p><code>runs-on</code>: <code>ubuntu-latest</code></p> <p>Steps:</p> <ol> <li>Checkout repository:</li> <li>Uses <code>actions/checkout@v4</code> to fetch the pull request's code for analysis.</li> <li>Dependency Review:</li> <li>Uses <code>actions/dependency-review-action@v4</code> to analyze changes to dependencies. This action checks for:<ul> <li>Newly added dependencies.</li> <li>Updates to existing dependencies.</li> <li>Security vulnerabilities in any affected dependencies.</li> </ul> </li> </ol> <p>In summary, the \"Dependency Review\" workflow enhances your project\u2019s security by automatically evaluating dependency changes in pull requests, helping you catch potential issues early in the development cycle.</p>"},{"location":"github_actions/docs/","title":"GitHub Action: Documentation Build and Deployment","text":"<p>This GitHub Action workflow automates the process of building and deploying project documentation using <code>mkdocs</code>. It ensures that documentation updates are automatically published whenever pull requests affecting specific files are merged or when triggered manually.</p> <p>docs.yml</p>"},{"location":"github_actions/docs/#workflow-details","title":"Workflow Details","text":""},{"location":"github_actions/docs/#triggers-on","title":"Triggers (<code>on</code>)","text":"<ul> <li><code>pull_request</code>:<ul> <li>Triggered when a pull request affecting documentation files is closed (merged) into the <code>main</code> branch.</li> <li>Monitored paths:<ul> <li>Files in the <code>docs/</code> directory.</li> <li><code>readme.md</code>.</li> </ul> </li> </ul> </li> <li><code>workflow_dispatch</code>: Allows manual triggering of the workflow via the GitHub user interface.</li> </ul>"},{"location":"github_actions/docs/#job-details","title":"Job Details","text":""},{"location":"github_actions/docs/#build-and-publish","title":"build-and-publish","text":"<p><code>runs-on</code>: <code>ubuntu-latest</code></p> <p>Steps:</p> <ol> <li>Checkout repository:</li> <li> <p>Uses <code>actions/checkout@v4</code> to fetch the repository contents, ensuring access to the latest documentation files.</p> </li> <li> <p>Install <code>uv</code>:</p> </li> <li> <p>Sets up the <code>uv</code> tool using <code>astral-sh/setup-uv@v6</code>.</p> </li> <li> <p>Set up Python:</p> </li> <li> <p>Configures Python according to the <code>.python-version</code> file using <code>actions/setup-python@v5</code>.</p> </li> <li> <p>Build documentation:</p> </li> <li> <p>Runs <code>mkdocs build --clean</code> via <code>uv</code> to generate the static documentation site. The <code>--clean</code> option ensures that the output directory is cleared before building.</p> </li> <li> <p>Deploy documentation:</p> </li> <li>Uses <code>peaceiris/actions-gh-pages@v4</code> to publish the built documentation to GitHub Pages.</li> <li>Inputs:<ul> <li><code>github_token</code>: A Personal Access Token (PAT) stored as a repository secret.</li> <li><code>publish_dir</code>: Specifies the directory containing the built site (<code>./site</code>).</li> </ul> </li> </ol> <p>In summary, this workflow streamlines the process of building and deploying documentation. It automatically updates GitHub Pages whenever documentation-related files are merged into the main branch, with the added flexibility of manual triggering for on-demand updates.</p>"},{"location":"github_actions/gh_action_pre-commit-autoupdate/","title":"GitHub Action:Pre-commit auto-update","text":"<p>This GitHub Action file configures a workflow named \"Pre-commit auto-update\". This workflow is responsible for automatically updating the pre-commit hooks in your repository.</p> <p>pre-commit_autoupdate.yml</p>"},{"location":"github_actions/gh_action_pre-commit-autoupdate/#workflow-details","title":"Workflow Details","text":"<ul> <li> <p><code>on</code>: This workflow is triggered in two situations:</p> </li> <li> <p><code>schedule</code>: It runs automatically every Monday at 7:00 UTC (according to the cron schedule <code>\"0 7 * * 1\"</code>).</p> </li> <li><code>workflow_dispatch</code>: This allows the workflow to be manually run from the GitHub user interface.</li> <li><code>jobs</code>: Defines a job named \"pre-commit-update\".</li> </ul>"},{"location":"github_actions/gh_action_pre-commit-autoupdate/#job-pre-commit-update-details","title":"Job \"pre-commit-update\" Details","text":"<p><code>runs-on</code>: This job runs on the latest version of Ubuntu.</p> <p><code>steps</code>: Defines the steps to be followed in this job.</p> <p><code>Checkout</code>: This step uses the actions/checkout@v3 action to get a copy of the repository.</p> <p><code>Update pre-commit hooks</code>: This step uses the <code>brokenpip3/action-pre-commit-update</code> action to update the pre-commit hooks. This action requires a GitHub token to function, which is passed through <code>github-token: ${{ secrets.PAT }}</code>. PAT is a secret stored in the repository settings that contains a personal access token with repository scope.</p> <p>In summary, this workflow takes care of keeping the pre-commit hooks in the repository up-to-date, running automatically every Monday and also allowing manual execution whenever necessary.</p>"},{"location":"github_actions/labels/","title":"GitHub Action: Label Synchronization","text":"<p>This GitHub Action workflow automates the synchronization of GitHub labels based on a YAML configuration file. It runs on changes to the label configuration or its workflow file and ensures consistency in label management across the repository.</p> <p>github-labeler.yml</p>"},{"location":"github_actions/labels/#workflow-details","title":"Workflow Details","text":""},{"location":"github_actions/labels/#triggers-on","title":"Triggers (<code>on</code>)","text":"<ul> <li><code>push</code>:<ul> <li>Triggered when changes are pushed to the <code>main</code> branch.</li> <li>Monitored paths:<ul> <li><code>.github/labels.yml</code>: The YAML file containing label definitions.</li> <li><code>.github/workflows/labels.yml</code>: The workflow configuration for label synchronization.</li> </ul> </li> </ul> </li> <li><code>pull_request</code>:<ul> <li>Triggered when pull requests modify the <code>.github/labels.yml</code> or <code>.github/workflows/labels.yml</code> files.</li> </ul> </li> </ul>"},{"location":"github_actions/labels/#job-details","title":"Job Details","text":""},{"location":"github_actions/labels/#labeler","title":"labeler","text":"<p><code>runs-on</code>: <code>ubuntu-latest</code></p> <p>Steps:</p> <ol> <li>Checkout repository:</li> <li> <p>Uses <code>actions/checkout@v4</code> to fetch the repository content.</p> </li> <li> <p>Run Labeler:</p> </li> <li>Uses <code>crazy-max/ghaction-github-labeler@v5</code> to synchronize labels based on the <code>.github/labels.yml</code> file.</li> <li>Inputs:<ul> <li><code>github-token</code>: A personal access token (PAT) stored in the repository's secrets for authentication.</li> <li><code>yaml-file</code>: Specifies the path to the label configuration file (<code>.github/labels.yml</code>).</li> <li><code>dry-run</code>: Simulates the label synchronization process when the workflow is triggered by a pull request (<code>true</code> for pull requests, <code>false</code> otherwise).</li> <li><code>exclude</code>: Excludes specific labels from synchronization. Here, labels matching <code>help*</code> or <code>*issue</code> are excluded.</li> </ul> </li> </ol> <p>In summary, this workflow ensures that GitHub labels in your repository remain consistent with the definitions in <code>.github/labels.yml</code>. It supports both live updates for pushes and simulation mode for pull requests, providing a robust solution for label management.</p>"},{"location":"es/","title":"Plantilla de proyecto de ciencia de datos","text":"<p>Una plantilla moderna para proyectos de ciencia de datos con todas las herramientas necesarias para experimentaci\u00f3n, desarrollo, pruebas y despliegue. Desde notebooks hasta producci\u00f3n.</p> <p>\u2728\ud83d\udcda\u2728 Documentaci\u00f3n: https://joserzapata.github.io/data-science-project-template/</p> <p>C\u00f3digo Fuente: https://github.com/JoseRZapata/data-science-project-template</p>"},{"location":"es/#caracteristicas","title":"Caracter\u00edsticas","text":"<ul> <li>Gesti\u00f3n de dependencias con UV</li> <li>Gesti\u00f3n de entornos virtuales con UV</li> <li>Linting con pre-commit y Ruff</li> <li>Integraci\u00f3n continua con GitHub Actions</li> <li>Documentaci\u00f3n con mkdocs y mkdocstrings usando el tema mkdocs-material</li> <li>Actualizaciones autom\u00e1ticas de dependencias con Dependabot</li> <li>Formateo de c\u00f3digo con Ruff</li> <li>Ordenamiento de imports con Ruff usando la regla isort.</li> <li>Pruebas con pytest</li> <li>Cobertura de c\u00f3digo con Coverage.py</li> <li>Reportes de cobertura con Codecov</li> <li>Verificaci\u00f3n est\u00e1tica de tipos con mypy</li> <li>Auditor\u00eda de seguridad con Ruff usando la regla bandit.</li> <li>Gesti\u00f3n de etiquetas del proyecto con GitHub Labeler</li> </ul> <ul> <li>Referencias</li> </ul>"},{"location":"es/#crear-un-nuevo-proyecto","title":"\ud83d\udcc1 Crear un Nuevo Proyecto","text":""},{"location":"es/#recomendaciones","title":"\ud83d\udc4d Recomendaciones","text":"<p>Se recomienda encarecidamente usar gestores para las versiones de Python, dependencias y entornos virtuales.</p> <p>Este proyecto usa UV, una herramienta extremadamente r\u00e1pida para reemplazar pip, pip-tools, Pipx, Poetry, Pyenv, twine, virtualenv, y m\u00e1s.</p> <p>\ud83c\udf1f Revisa c\u00f3mo configurar tu entorno: https://joserzapata.github.io/data-science-project-template/local_setup/</p>"},{"location":"es/#via-cruft-recomendado","title":"\ud83c\udf6a\ud83e\udd47 V\u00eda Cruft - (recomendado)","text":"instalar cruft<pre><code># Instalar cruft en un entorno aislado usando uv\n\nuv tool install cruft\n\n# O instalar con pip\n\npip install --user cruft # Instalar `cruft` en tu PATH para f\u00e1cil acceso\n</code></pre> crear proyecto<pre><code>cruft create https://github.com/JoseRZapata/data-science-project-template\n</code></pre> <p>luego dentro de la carpeta del proyecto, inicializar git y el entorno uv usando Make:</p> instalar proyecto<pre><code>make init_git\nmake install_env\nsource .venv/bin/activate\n</code></pre>"},{"location":"es/#via-cookiecutter","title":"\ud83c\udf6a V\u00eda Cookiecutter","text":"instalar cookiecutter<pre><code>uv tool install cookiecutter # Instalar cookiecutter en un entorno aislado\n\n# O instalar con pip\n\npip install --user cookiecutter # Instalar `cookiecutter` en tu PATH para f\u00e1cil acceso\n</code></pre> crear proyecto<pre><code>cookiecutter gh:JoseRZapata/data-science-project-template\n</code></pre> <p>Nota: Cookiecutter usa <code>gh:</code> como abreviatura de <code>https://github.com/</code></p>"},{"location":"es/#vincular-un-proyecto-existente","title":"\ud83d\udd17  Vincular un Proyecto Existente","text":"<p>Si el proyecto fue instalado originalmente v\u00eda Cookiecutter, primero debes usar Cruft para vincular el proyecto con la plantilla original:</p> <pre><code>cruft link https://github.com/JoseRZapata/data-science-project-template\n</code></pre> <p>Luego:</p> <pre><code>cruft update\n</code></pre>"},{"location":"es/#estructura-del-proyecto","title":"\ud83d\uddc3\ufe0f Estructura del proyecto","text":"<p>Estructura de carpetas para proyectos de ciencia de datos  \u00bfpor qu\u00e9?</p> <ul> <li>Estructura de datos</li> <li>Pipelines basados en Feature/Training/Inference Pipelines</li> </ul> <pre><code>.\n\u251c\u2500\u2500 .code_quality\n\u2502   \u251c\u2500\u2500 mypy.ini                        # configuraci\u00f3n de mypy\n\u2502   \u2514\u2500\u2500 ruff.toml                       # configuraci\u00f3n de ruff\n\u251c\u2500\u2500 .github                             # configuraci\u00f3n de github\n\u2502   \u251c\u2500\u2500 actions\n\u2502   \u2502   \u2514\u2500\u2500 python-poetry-env\n\u2502   \u2502       \u2514\u2500\u2500 action.yml              # github action para configurar entorno python\n\u2502   \u251c\u2500\u2500 dependabot.md                   # github action para actualizar dependencias\n\u2502   \u251c\u2500\u2500 pull_request_template.md        # plantilla para pull requests\n\u2502   \u2514\u2500\u2500 workflows                       # flujos de trabajo de github actions\n\u2502       \u251c\u2500\u2500 ci.yml                      # ejecutar integraci\u00f3n continua (pruebas, pre-commit, etc.)\n\u2502       \u251c\u2500\u2500 dependency_review.yml       # revisi\u00f3n de dependencias\n\u2502       \u251c\u2500\u2500 docs.yml                    # construir documentaci\u00f3n (mkdocs)\n\u2502       \u2514\u2500\u2500 pre-commit_autoupdate.yml   # actualizar hooks de pre-commit\n\u251c\u2500\u2500 .vscode                             # configuraci\u00f3n de vscode\n|   \u251c\u2500\u2500 extensions.json                 # lista de extensiones recomendadas\n|   \u251c\u2500\u2500 launch.json                     # configuraci\u00f3n de ejecuci\u00f3n de vscode\n|   \u2514\u2500\u2500 settings.json                   # configuraci\u00f3n de vscode\n\u251c\u2500\u2500 conf                                # carpeta de archivos de configuraci\u00f3n\n\u2502   \u2514\u2500\u2500 config.yaml                     # archivo principal de configuraci\u00f3n\n\u251c\u2500\u2500 data\n\u2502   \u251c\u2500\u2500 01_raw                          # datos crudos inmutables\n\u2502   \u251c\u2500\u2500 02_intermediate                 # datos tipados\n\u2502   \u251c\u2500\u2500 03_primary                      # datos del modelo de dominio\n\u2502   \u251c\u2500\u2500 04_feature                      # caracter\u00edsticas del modelo\n\u2502   \u251c\u2500\u2500 05_model_input                  # frecuentemente llamados 'master tables'\n\u2502   \u251c\u2500\u2500 06_models                       # modelos serializados\n\u2502   \u251c\u2500\u2500 07_model_output                 # datos generados por ejecuciones del modelo\n\u2502   \u251c\u2500\u2500 08_reporting                    # reportes, resultados, etc\n\u2502   \u2514\u2500\u2500 README.md                       # descripci\u00f3n de la estructura de datos\n\u251c\u2500\u2500 docs                                # documentaci\u00f3n de tu proyecto\n\u2502   \u251c\u2500\u2500 index.md                        # p\u00e1gina principal de documentaci\u00f3n\n\u251c\u2500\u2500 models                              # almacenar modelos finales\n\u251c\u2500\u2500 notebooks\n\u2502   \u251c\u2500\u2500 1-data                          # extracci\u00f3n y limpieza de datos\n\u2502   \u251c\u2500\u2500 2-exploration                   # an\u00e1lisis exploratorio de datos (EDA)\n\u2502   \u251c\u2500\u2500 3-analysis                      # An\u00e1lisis estad\u00edstico, pruebas de hip\u00f3tesis.\n\u2502   \u251c\u2500\u2500 4-feat_eng                      # ingenier\u00eda de caracter\u00edsticas (creaci\u00f3n, selecci\u00f3n y transformaci\u00f3n.)\n\u2502   \u251c\u2500\u2500 5-models                        # entrenamiento de modelos, evaluaci\u00f3n y ajuste de hiperpar\u00e1metros.\n\u2502   \u251c\u2500\u2500 6-interpretation                # interpretaci\u00f3n de modelos\n\u2502   \u251c\u2500\u2500 7-deploy                        # empaquetado de modelos, estrategias de despliegue.\n\u2502   \u251c\u2500\u2500 8-reports                       # narrativa, res\u00famenes y conclusiones de an\u00e1lisis.\n\u2502   \u251c\u2500\u2500 notebook_template.ipynb         # plantilla para notebooks\n\u2502   \u2514\u2500\u2500 README.md                       # informaci\u00f3n sobre los notebooks\n\u251c\u2500\u2500 src                                 # c\u00f3digo fuente para uso en este proyecto\n\u2502   \u251c\u2500\u2500 README.md                       # descripci\u00f3n de la estructura de src\n\u2502   \u251c\u2500\u2500 tmp_mock.py                     # archivo python de ejemplo\n\u2502   \u251c\u2500\u2500 data                            # extracci\u00f3n, validaci\u00f3n, procesamiento, transformaci\u00f3n de datos\n\u2502   \u251c\u2500\u2500 model                           # entrenamiento, evaluaci\u00f3n, validaci\u00f3n, exportaci\u00f3n de modelos\n\u2502   \u251c\u2500\u2500 inference                       # predicci\u00f3n, servicio, monitoreo de modelos\n\u2502   \u2514\u2500\u2500 pipelines                       # orquestaci\u00f3n de pipelines\n\u2502       \u251c\u2500\u2500 feature_pipeline            # transforma datos crudos en caracter\u00edsticas y etiquetas\n\u2502       \u251c\u2500\u2500 training_pipeline           # transforma caracter\u00edsticas y etiquetas en un modelo\n\u2502       \u2514\u2500\u2500 inference_pipeline          # toma caracter\u00edsticas y un modelo entrenado para predicciones\n\u251c\u2500\u2500 tests                               # c\u00f3digo de pruebas para tu proyecto\n\u2502   \u251c\u2500\u2500 test_mock.py                    # archivo de prueba de ejemplo\n\u2502   \u251c\u2500\u2500 data                            # pruebas para el m\u00f3dulo data\n\u2502   \u251c\u2500\u2500 model                           # pruebas para el m\u00f3dulo model\n\u2502   \u251c\u2500\u2500 inference                       # pruebas para el m\u00f3dulo inference\n\u2502   \u2514\u2500\u2500 pipelines                       # pruebas para el m\u00f3dulo pipelines\n\u251c\u2500\u2500 .editorconfig                       # configuraci\u00f3n del editor\n\u251c\u2500\u2500 .gitignore                          # archivos a ignorar en git\n\u251c\u2500\u2500 .pre-commit-config.yaml             # configuraci\u00f3n para hooks de pre-commit\n\u251c\u2500\u2500 codecov.yml                         # configuraci\u00f3n para codecov\n\u251c\u2500\u2500 Makefile                            # comandos \u00fatiles para configurar entorno, ejecutar pruebas, etc.\n\u251c\u2500\u2500 mkdocs.yml                          # configuraci\u00f3n para documentaci\u00f3n mkdocs\n\u251c\u2500\u2500 pyproject.toml                      # archivo de dependencias y configuraci\u00f3n del proyecto\n\u251c\u2500\u2500 uv.lock                             # dependencias bloqueadas\n\u2514\u2500\u2500 README.md                           # descripci\u00f3n de tu proyecto\n</code></pre>"},{"location":"es/#caracteristicas-y-herramientas","title":"\u2728 Caracter\u00edsticas y Herramientas","text":""},{"location":"es/#estandarizacion-y-automatizacion-del-proyecto","title":"\ud83d\ude80 Estandarizaci\u00f3n y Automatizaci\u00f3n del Proyecto","text":""},{"location":"es/#automatizacion-del-flujo-de-trabajo-del-desarrollador","title":"\ud83d\udd28 Automatizaci\u00f3n del Flujo de Trabajo del Desarrollador","text":"<ul> <li>Empaquetado de Python, gesti\u00f3n de dependencias y gesti\u00f3n de entornos   con UV - <code>\u00bfpor qu\u00e9 usar un gestor? (uv es un reemplazo de poetry)</code></li> <li>Orquestaci\u00f3n del flujo de trabajo del proyecto   con Make como interfaz shim<ul> <li>Makefile autodocumentado; simplemente escribe   <code>make</code> en la l\u00ednea de comandos para mostrar documentaci\u00f3n autogenerada sobre los   objetivos disponibles:</li> </ul> </li> <li>Sincronizaci\u00f3n automatizada de plantillas Cookiecutter con Cruft - <code>\u00bfpor qu\u00e9?</code></li> <li>Automatizaci\u00f3n y gesti\u00f3n de herramientas de calidad de c\u00f3digo con pre-commit</li> <li>Integraci\u00f3n y despliegue continuos con GitHub Actions</li> <li>Archivos de configuraci\u00f3n del proyecto con Hydra - <code>\u00bfpor qu\u00e9?</code></li> </ul>"},{"location":"es/#paquete-python-o-plantilla-de-proyecto-renderizado-condicionalmente","title":"\ud83c\udf31 Paquete Python o Plantilla de Proyecto Renderizado Condicionalmente","text":"<ul> <li>Opcional: Soporte para Jupyter</li> </ul>"},{"location":"es/#mantenibilidad","title":"\ud83d\udd27 Mantenibilidad","text":""},{"location":"es/#verificacion-de-tipos-y-validacion-de-datos","title":"\ud83c\udff7\ufe0f  Verificaci\u00f3n de Tipos y Validaci\u00f3n de Datos","text":"<ul> <li>Verificaci\u00f3n est\u00e1tica de tipos con Mypy</li> </ul>"},{"location":"es/#pruebascobertura","title":"\u2705 \ud83e\uddea Pruebas/Cobertura","text":"<ul> <li>Pruebas con Pytest</li> <li>Cobertura de c\u00f3digo con Coverage.py</li> <li>Reportes de cobertura con Codecov</li> </ul>"},{"location":"es/#calidad-de-codigo","title":"\ud83d\udd0d Calidad de c\u00f3digo","text":"<ul> <li>Ruff Un linter y formateador de Python extremadamente r\u00e1pido (10x-100x m\u00e1s r\u00e1pido), escrito en Rust.<ul> <li>Reemplazo de ~~Pylint~~, ~~Flake8~~ (incluyendo plugins principales) y m\u00e1s linters bajo una interfaz \u00fanica y com\u00fan</li> </ul> </li> <li>ShellCheck</li> <li>Commits no seguros:<ul> <li>Secretos con <code>detect-secrets</code></li> <li>Archivos grandes con <code>check-added-large-files</code></li> <li>Archivos que contienen cadenas de conflicto de merge. check-merge-conflict</li> </ul> </li> </ul>"},{"location":"es/#formateo-de-codigo","title":"\ud83c\udfa8 Formateo de c\u00f3digo","text":"<ul> <li> <p>Ruff Un linter y formateador de Python extremadamente r\u00e1pido (10x-100x m\u00e1s r\u00e1pido), escrito en Rust.</p> <ul> <li>Reemplazo de ~~Black~~, ~~isort~~, ~~pyupgrade~~ y m\u00e1s formateadores bajo una interfaz \u00fanica y com\u00fan</li> </ul> </li> <li> <p>Formateo general de archivos:</p> <ul> <li><code>end-of-file-fixer</code></li> <li><code>pretty-format-json</code></li> <li>(trim) <code>trailing-whitespace</code></li> <li><code>check-yaml</code></li> </ul> </li> </ul>"},{"location":"es/#actualizaciones-automaticas-de-dependencias","title":"Actualizaciones autom\u00e1ticas de dependencias","text":"<ul> <li> <p>Actualizaciones de dependencias con Dependabot, merge automatizado de PRs de Dependabot con el Dependabot Auto Merge GitHub Action</p> </li> <li> <p>Esto es un reemplazo de pip-audit, En tu entorno local, si quieres verificar vulnerabilidades en tus dependencias puedes usar [pip-audit].</p> </li> </ul>"},{"location":"es/#revision-de-dependencias-en-pr","title":"Revisi\u00f3n de dependencias en PR","text":"<ul> <li>Revisi\u00f3n de dependencias con dependency-review-action, esta acci\u00f3n escanea tus pull requests por cambios de dependencias, y generar\u00e1 un error si se est\u00e1n introduciendo vulnerabilidades o licencias inv\u00e1lidas.</li> </ul>"},{"location":"es/#actualizaciones-automaticas-de-pre-commit","title":"Actualizaciones autom\u00e1ticas de Pre-commit","text":"<ul> <li>Actualizaciones autom\u00e1ticas con flujo de trabajo de GitHub Actions <code>.github/workflows/pre-commit_autoupdate.yml</code></li> </ul>"},{"location":"es/#seguridad","title":"\ud83d\udd12 Seguridad","text":""},{"location":"es/#pruebas-estaticas-de-seguridad-de-aplicaciones-sast","title":"\ud83d\udd0f Pruebas Est\u00e1ticas de Seguridad de Aplicaciones (SAST)","text":"<ul> <li>Vulnerabilidades de c\u00f3digo con Bandit usando Ruff</li> </ul>"},{"location":"es/#accesibilidad","title":"\u2328\ufe0f Accesibilidad","text":""},{"location":"es/#herramienta-de-automatizacion-makefile","title":"\ud83d\udd28 Herramienta de automatizaci\u00f3n (Makefile)","text":"<p>Makefile para automatizar la configuraci\u00f3n de tu entorno, la instalaci\u00f3n de dependencias, la ejecuci\u00f3n de pruebas, etc. en la terminal escribe <code>make</code> para ver los comandos disponibles</p> <pre><code>Target                Descripci\u00f3n\n-------------------   ----------------------------------------------------\ncheck                 Ejecutar herramientas de calidad de c\u00f3digo con hooks de pre-commit.\ndocs_test             Probar si la documentaci\u00f3n se puede construir sin advertencias o errores\ndocs_view             Construir y servir la documentaci\u00f3n\ninit_env              Instalar dependencias con uv y activar entorno\ninit_git              Inicializar repositorio git\ninstall_data_libs     Instalar pandas, scikit-learn, Jupyter, seaborn\npre-commit_update     Actualizar hooks de pre-commit\ntest                  Probar el c\u00f3digo con pytest y cobertura\n</code></pre>"},{"location":"es/#documentacion-del-proyecto","title":"\ud83d\udcdd Documentaci\u00f3n del Proyecto","text":"<ul> <li>Construcci\u00f3n de documentaci\u00f3n   con MkDocs - Tutorial<ul> <li>Potenciado por mkdocs-material</li> <li>Documentaci\u00f3n autom\u00e1tica rica a partir de anotaciones de tipo y docstrings (NumPy, Google, etc.) con mkdocstrings</li> </ul> </li> </ul>"},{"location":"es/#plantillas","title":"\ud83d\uddc3\ufe0f Plantillas","text":"<ul> <li>Plantilla de Pull Request</li> <li>Plantilla de Notebook</li> </ul>"},{"location":"es/#buenas-practicas","title":"Buenas pr\u00e1cticas","text":"<ul> <li>https://www.conventionalcommits.org/</li> <li>https://keepachangelog.com/</li> </ul>"},{"location":"es/#referencias","title":"Referencias","text":"<ul> <li>https://drivendata.github.io/cookiecutter-data-science/</li> <li>https://github.com/crmne/cookiecutter-modern-datascience</li> <li>https://github.com/fpgmaas/cookiecutter-poetry</li> <li>https://github.com/khuyentran1401/data-science-template</li> <li>https://github.com/woltapp/wolt-python-package-cookiecutter</li> <li>https://khuyentran1401.github.io/reproducible-data-science/structure_project/introduction.html</li> <li>https://github.com/TeoZosa/cookiecutter-cruft-poetry-tox-pre-commit-ci-cd</li> <li>https://github.com/cjolowicz/cookiecutter-hypermodern-python</li> <li>https://github.com/gotofritz/cookiecutter-gotofritz-poetry</li> <li>https://github.com/kedro-org/kedro-starters</li> </ul>"},{"location":"es/data_schema/","title":"Estructura de datos","text":"<p>Convenci\u00f3n de ingenier\u00eda de datos por capas</p> <p></p> <code>Carpeta en data</code> <code>Descripci\u00f3n</code> <code>raw</code> inicio del pipeline, contiene el/los modelo(s) de datos de origen que nunca deben ser modificados, forman tu \u00fanica fuente de verdad. Estos modelos de datos t\u00edpicamente no tienen tipo en la mayor\u00eda de los casos, ej. csv, pero esto var\u00eda de caso a caso <code>intermediate</code> modelo(s) de datos opcionales, que se introducen para tipar tu(s) modelo(s) de datos crudos, ej. convirtiendo valores basados en cadenas de texto a su representaci\u00f3n tipada actual <code>primary</code> modelo(s) de datos espec\u00edficos del dominio que contienen datos limpiados, transformados y procesados desde raw o intermediate, que forman la capa de entrada para tu ingenier\u00eda de caracter\u00edsticas <code>feature</code> modelo(s) de datos espec\u00edficos de an\u00e1lisis que contienen un conjunto de caracter\u00edsticas definidas contra los datos primarios, agrupados por \u00e1rea de an\u00e1lisis y almacenados contra una dimensi\u00f3n com\u00fan <code>model input</code> modelo(s) de datos espec\u00edficos de an\u00e1lisis que contienen todos los datos de caracter\u00edsticas contra una dimensi\u00f3n com\u00fan y, en el caso de proyectos en producci\u00f3n, contra una fecha de ejecuci\u00f3n de an\u00e1lisis para asegurar el seguimiento de cambios hist\u00f3ricos de las caracter\u00edsticas a lo largo del tiempo <code>models</code> modelos de aprendizaje autom\u00e1tico pre-entrenados almacenados y serializados <code>model output</code> modelo(s) de datos espec\u00edficos de an\u00e1lisis que contienen los resultados generados por el modelo basados en los datos de entrada del modelo <code>reporting</code> modelo(s) de datos de reportes que se usan para combinar un conjunto de datos primarios, caracter\u00edsticas, entrada del modelo y salida del modelo usados para construir el dashboard y las vistas. Encapsula y elimina la necesidad de definir cualquier mezcla o uni\u00f3n de datos, mejora el rendimiento y el reemplazo de la capa de presentaci\u00f3n sin tener que redefinir los modelos de datos"},{"location":"es/data_schema/#referencias","title":"Referencias","text":"<ul> <li>https://docs.kedro.org/en/stable/faq/faq.html#what-is-data-engineering-convention</li> <li>https://towardsdatascience.com/the-importance-of-layered-thinking-in-data-engineering-a09f685edc71</li> </ul>"},{"location":"es/directory_hierarchy/","title":"\ud83d\uddc2\ufe0f Jerarqu\u00eda de directorios","text":"<p>Estructura de carpetas para proyectos de ciencia de datos  \u00bfpor qu\u00e9?</p> <ul> <li>Estructura de datos</li> <li>Pipelines basados en Feature/Training/Inference Pipelines</li> </ul> <pre><code>.\n\u251c\u2500\u2500 .code_quality\n\u2502   \u251c\u2500\u2500 mypy.ini                        # configuraci\u00f3n de mypy\n\u2502   \u2514\u2500\u2500 ruff.toml                       # configuraci\u00f3n de ruff\n\u251c\u2500\u2500 .github                             # configuraci\u00f3n de github\n\u2502   \u251c\u2500\u2500 actions\n\u2502   \u2502   \u2514\u2500\u2500 python-poetry-env\n\u2502   \u2502       \u2514\u2500\u2500 action.yml              # github action para configurar entorno python\n\u2502   \u251c\u2500\u2500 dependabot.md                   # github action para actualizar dependencias\n\u2502   \u251c\u2500\u2500 pull_request_template.md        # plantilla para pull requests\n\u2502   \u2514\u2500\u2500 workflows                       # flujos de trabajo de github actions\n\u2502       \u251c\u2500\u2500 ci.yml                      # ejecutar integraci\u00f3n continua (pruebas, pre-commit, etc.)\n\u2502       \u251c\u2500\u2500 dependency_review.yml       # revisi\u00f3n de dependencias\n\u2502       \u251c\u2500\u2500 docs.yml                    # construir documentaci\u00f3n (mkdocs)\n\u2502       \u2514\u2500\u2500 pre-commit_autoupdate.yml   # actualizar hooks de pre-commit\n\u251c\u2500\u2500 .vscode                             # configuraci\u00f3n de vscode\n|   \u251c\u2500\u2500 extensions.json                 # lista de extensiones recomendadas\n|   \u251c\u2500\u2500 launch.json                     # configuraci\u00f3n de ejecuci\u00f3n de vscode\n|   \u2514\u2500\u2500 settings.json                   # configuraci\u00f3n de vscode\n\u251c\u2500\u2500 conf                                # carpeta de archivos de configuraci\u00f3n\n\u2502   \u2514\u2500\u2500 config.yaml                     # archivo principal de configuraci\u00f3n\n\u251c\u2500\u2500 data\n\u2502   \u251c\u2500\u2500 01_raw                          # datos crudos inmutables\n\u2502   \u251c\u2500\u2500 02_intermediate                 # datos tipados\n\u2502   \u251c\u2500\u2500 03_primary                      # datos del modelo de dominio\n\u2502   \u251c\u2500\u2500 04_feature                      # caracter\u00edsticas del modelo\n\u2502   \u251c\u2500\u2500 05_model_input                  # frecuentemente llamados 'master tables'\n\u2502   \u251c\u2500\u2500 06_models                       # modelos serializados\n\u2502   \u251c\u2500\u2500 07_model_output                 # datos generados por ejecuciones del modelo\n\u2502   \u251c\u2500\u2500 08_reporting                    # reportes, resultados, etc\n\u2502   \u2514\u2500\u2500 README.md                       # descripci\u00f3n de la estructura de datos\n\u251c\u2500\u2500 docs                                # documentaci\u00f3n de tu proyecto\n\u2502   \u251c\u2500\u2500 index.md                        # p\u00e1gina principal de documentaci\u00f3n\n\u251c\u2500\u2500 models                              # almacenar modelos finales\n\u251c\u2500\u2500 notebooks\n\u2502   \u251c\u2500\u2500 1-data                          # extracci\u00f3n y limpieza de datos\n\u2502   \u251c\u2500\u2500 2-exploration                   # an\u00e1lisis exploratorio de datos (EDA)\n\u2502   \u251c\u2500\u2500 3-analysis                      # An\u00e1lisis estad\u00edstico, pruebas de hip\u00f3tesis.\n\u2502   \u251c\u2500\u2500 4-feat_eng                      # ingenier\u00eda de caracter\u00edsticas (creaci\u00f3n, selecci\u00f3n y transformaci\u00f3n.)\n\u2502   \u251c\u2500\u2500 5-models                        # entrenamiento de modelos, evaluaci\u00f3n y ajuste de hiperpar\u00e1metros.\n\u2502   \u251c\u2500\u2500 6-interpretation                # interpretaci\u00f3n de modelos\n\u2502   \u251c\u2500\u2500 7-deploy                        # empaquetado de modelos, estrategias de despliegue.\n\u2502   \u251c\u2500\u2500 8-reports                       # narrativa, res\u00famenes y conclusiones de an\u00e1lisis.\n\u2502   \u251c\u2500\u2500 notebook_template.ipynb         # plantilla para notebooks\n\u2502   \u2514\u2500\u2500 README.md                       # informaci\u00f3n sobre los notebooks\n\u251c\u2500\u2500 src                                 # c\u00f3digo fuente para uso en este proyecto\n\u2502   \u251c\u2500\u2500 README.md                       # descripci\u00f3n de la estructura de src\n\u2502   \u251c\u2500\u2500 tmp_mock.py                     # archivo python de ejemplo\n\u2502   \u251c\u2500\u2500 data                            # extracci\u00f3n, validaci\u00f3n, procesamiento, transformaci\u00f3n de datos\n\u2502   \u251c\u2500\u2500 model                           # entrenamiento, evaluaci\u00f3n, validaci\u00f3n, exportaci\u00f3n de modelos\n\u2502   \u251c\u2500\u2500 inference                       # predicci\u00f3n, servicio, monitoreo de modelos\n\u2502   \u2514\u2500\u2500 pipelines                       # orquestaci\u00f3n de pipelines\n\u2502       \u251c\u2500\u2500 feature_pipeline            # transforma datos crudos en caracter\u00edsticas y etiquetas\n\u2502       \u251c\u2500\u2500 training_pipeline           # transforma caracter\u00edsticas y etiquetas en un modelo\n\u2502       \u2514\u2500\u2500 inference_pipeline          # toma caracter\u00edsticas y un modelo entrenado para predicciones\n\u251c\u2500\u2500 tests                               # c\u00f3digo de pruebas para tu proyecto\n\u2502   \u251c\u2500\u2500 test_mock.py                    # archivo de prueba de ejemplo\n\u2502   \u251c\u2500\u2500 data                            # pruebas para el m\u00f3dulo data\n\u2502   \u251c\u2500\u2500 model                           # pruebas para el m\u00f3dulo model\n\u2502   \u251c\u2500\u2500 inference                       # pruebas para el m\u00f3dulo inference\n\u2502   \u2514\u2500\u2500 pipelines                       # pruebas para el m\u00f3dulo pipelines\n\u251c\u2500\u2500 .editorconfig                       # configuraci\u00f3n del editor\n\u251c\u2500\u2500 .gitignore                          # archivos a ignorar en git\n\u251c\u2500\u2500 .pre-commit-config.yaml             # configuraci\u00f3n para hooks de pre-commit\n\u251c\u2500\u2500 codecov.yml                         # configuraci\u00f3n para codecov\n\u251c\u2500\u2500 Makefile                            # comandos \u00fatiles para configurar entorno, ejecutar pruebas, etc.\n\u251c\u2500\u2500 mkdocs.yml                          # configuraci\u00f3n para documentaci\u00f3n mkdocs\n\u251c\u2500\u2500 pyproject.toml                      # archivo de dependencias y configuraci\u00f3n del proyecto\n\u251c\u2500\u2500 uv.lock                             # dependencias bloqueadas\n\u2514\u2500\u2500 README.md                           # descripci\u00f3n de tu proyecto\n</code></pre>"},{"location":"es/local_setup/","title":"\ud83d\udee0\ufe0f Configuraci\u00f3n del entorno de desarrollo local","text":"<p>Desarrollo proyectos de ciencia de datos en Python en Linux OS o MAC OS. (Para Windows OS recomiendo WSL y ejecutar los comandos como en Linux OS).</p> <p>Configuro mi entorno de desarrollo local siguiendo los siguientes pasos:</p>"},{"location":"es/local_setup/#configuracion-del-computador-para-desarrollar-con-python","title":"\ud83d\udcbb Configuraci\u00f3n del computador para desarrollar con Python","text":"<ol> <li>Instalar Git<ul> <li>Linux: <code>sudo apt-get install git</code></li> <li>MAC: <code>brew install git</code></li> </ul> </li> <li>Instalar Make<ul> <li>Linux: <code>sudo apt-get install make</code></li> <li>MAC: <code>brew install make</code></li> </ul> </li> <li> <p>Instalar localmente UV para gestionar versiones de Python, dependencias y entornos virtuales.</p> <ul> <li> <p>Linux:</p> Instalar uv en Linux o MACOS<pre><code>curl -LsSf https://astral.sh/uv/install.sh | sh\n</code></pre> <ul> <li>Verificar la versi\u00f3n de instalaci\u00f3n ejecutando en terminal: <code>uv version</code></li> </ul> </li> </ul> </li> <li> <p>Instalar Python usando UV, actualmente estoy usando Python 3.11</p> <ul> <li><code>uv python install 3.11</code> # Instalar Python 3.11 en el computador</li> </ul> </li> <li> <p>En la carpeta del proyecto, establecer la versi\u00f3n de Python y crear un entorno virtual.</p> </li> <li>Si el proyecto tiene archivo <code>uv.lock</code><ul> <li><code>uv sync</code> # Crear Entorno Virtual e instalar dependencias.</li> <li><code>source .venv/bin/activate</code> # Activar Entorno    2.</li> <li><code>uv venv --python 3.11</code> # Crear un entorno virtual de Python 3.11</li> <li><code>source .venv/bin/activate</code> # Activar el entorno virtual de Python 3.11</li> <li>Verificar la versi\u00f3n de instalaci\u00f3n ejecutando en terminal: <code>python --version</code></li> </ul> </li> </ol> <p>Nota: Para desactivar el entorno virtual en terminal escribe: <code>deactivate</code></p>"},{"location":"es/local_setup/#herramientas-generales-de-python","title":"\ud83d\udc0d Herramientas generales de Python","text":"<p>Herramientas generales que uso para desarrollar proyectos en Python. La m\u00e1s importante es UV para gestionar versiones de Python, entornos virtuales, gestionar dependencias para los proyectos y tener herramientas en entornos aislados, porque las aplicaciones se ejecutan en su propio entorno virtual para evitar conflictos de dependencias y est\u00e1n disponibles en todas partes.</p> <p>UV Highlights</p> <ol> <li>Cruft te permite mantener todo el c\u00f3digo repetitivo necesario para empaquetar y construir proyectos separado del c\u00f3digo que escribes intencionalmente. Totalmente compatible con plantillas existentes de Cookiecutter.<ul> <li><code>uv tool install cruft</code></li> </ul> </li> <li>(opcional) Pip-audit para verificar localmente la seguridad de las dependencias del proyecto.<ul> <li><code>uv tool install pip-audit</code></li> </ul> </li> <li>(opcional) Actionlint para verificar la sintaxis de los archivos de configuraci\u00f3n de GitHub Actions del proyecto.<ul> <li><code>uv tool install actionlint</code></li> </ul> </li> </ol> <p>Nota: UV reemplaza herramientas como Pyenv, Poetry y otras para gestionar las versiones de Python, entornos y dependencias.</p>"},{"location":"es/local_setup/#iniciar-un-nuevo-proyecto-de-ciencia-de-datos","title":"\ud83d\udcc1 Iniciar un nuevo proyecto de ciencia de datos","text":"<ol> <li> <p>Iniciar un nuevo proyecto usando Cruft con la Data Science Project Template.</p> crear proyecto<pre><code>cruft create https://github.com/JoseRZapata/data-science-project-template\n</code></pre> </li> <li> <p>Responder las preguntas para crear el proyecto.</p> </li> <li> <p>Ejecutar <code>make init_git</code> para inicializar Git o puedes hacer lo mismo ejecutando estos comandos:</p> inicializar entorno<pre><code>    git init -b main\n    git add .\n    git commit -m \"\ud83c\udf89 Begin a project, Initial commit\"\n</code></pre> </li> <li> <p>Ejecutar <code>make install_env</code> para instalar las dependencias del proyecto. O puedes hacer lo mismo ejecutando estos comandos:</p> instalar dependencias<pre><code>    uv sync --all-groups\n    uv run pre-commit install\n</code></pre> </li> <li> <p>\ud83c\udf89 \u00a1Felicidades! Comienza a programar tu proyecto.</p> </li> <li> <p>Si quieres subir este repositorio a GitHub, primero crea un nuevo repositorio en GitHub y luego ejecuta los siguientes comandos para subir tu repositorio local a GitHub.</p> subir a GitHub<pre><code>    git remote add origin &lt;tu-url-del-repo&gt;\n    git branch -M main\n    git push -u origin main\n</code></pre> </li> </ol>"},{"location":"es/pre-commit/","title":"Configuraci\u00f3n de Pre-commit","text":"<p>Este proyecto usa pre-commit para ejecutar verificaciones en cada commit.</p> <p>Archivo de configuraci\u00f3n: .pre-commit-config.yaml</p> <p>Si inicializas tu proyecto usando <code>make install</code> pre-commit ya est\u00e1 instalado y configurado. Si no, puedes instalar pre-commit ejecutando en terminal <code>uv run pre-commit install</code> en la ra\u00edz del proyecto.</p>"},{"location":"es/pre-commit/#pre-commitpre-commit-hooks","title":"pre-commit/pre-commit-hooks","text":"<p>Este repositorio contiene algunos hooks listos para usar proporcionados por el proyecto pre-commit.</p> <ul> <li><code>trailing-whitespace</code>: Este hook elimina los espacios en blanco al final de las l\u00edneas.</li> <li><code>end-of-file-fixer</code>: Este hook asegura que un archivo est\u00e9 vac\u00edo o termine con una nueva l\u00ednea.</li> <li><code>check-yaml</code>: Este hook verifica que los archivos yaml tengan una sintaxis v\u00e1lida.</li> <li><code>check-case-conflict</code>: Este hook verifica archivos con nombres que entrar\u00edan en conflicto en un sistema de archivos insensible a may\u00fasculas como MacOS HFS+ o Windows FAT.</li> <li><code>debug-statements</code>: Este hook verifica declaraciones de depuraci\u00f3n en Python.</li> <li><code>detect-private-key</code>: Este hook verifica la adici\u00f3n de claves privadas.</li> <li><code>check-merge-conflict</code>: Este hook verifica archivos que contienen cadenas de conflicto de merge.</li> <li><code>check-ast</code>: Este hook verifica que los archivos fuente de Python tengan un ast sint\u00e1cticamente v\u00e1lido.</li> <li><code>check-added-large-files</code>: Este hook previene agregar archivos grandes. El tama\u00f1o m\u00e1ximo es configurable.</li> </ul>"},{"location":"es/pre-commit/#astral-shruff-pre-commit","title":"astral-sh/ruff-pre-commit","text":"<p>Este repositorio contiene hooks para el lenguaje de programaci\u00f3n Ruff.</p> <ul> <li><code>ruff</code>: Este hook ejecuta el linter Ruff con la opci\u00f3n <code>--fix</code> para corregir autom\u00e1ticamente problemas en scripts y notebooks y un archivo de configuraci\u00f3n personalizado.</li> <li><code>ruff-format</code>: Este hook ejecuta el formateador Ruff.</li> </ul>"},{"location":"es/pre-commit/#pre-commitmirrors-mypy","title":"pre-commit/mirrors-mypy","text":"<p>Este repositorio contiene un mirror de mypy para pre-commit.</p> <ul> <li><code>mypy</code>: Este hook ejecuta mypy, un verificador est\u00e1tico de tipos para Python, con un archivo de configuraci\u00f3n personalizado.</li> </ul>"},{"location":"es/pre-commit/#commitizen-toolscommitizen","title":"commitizen-tools/commitizen","text":"<p>Este repositorio contiene hooks para commitizen.</p> <ul> <li><code>commitizen</code>: Este hook verifica que los mensajes de commit sigan el formato de commits convencionales.</li> </ul>"},{"location":"es/setup_tokens/","title":"\ud83d\udd11 Configurar Tokens","text":"<p>Algunas de las GitHub Actions requieren claves de token que se establezcan como secretos en el repositorio de GitHub. Los siguientes tokens son requeridos:</p>"},{"location":"es/setup_tokens/#github-action-secretspat","title":"GitHub Action secrets.PAT","text":"<p>Este es el token de acceso personal para el repositorio de GitHub y se usa para:</p> <ul> <li>Subir la documentaci\u00f3n de MKDocs a la rama <code>gh-pages</code>.</li> <li>Subir la actualizaci\u00f3n autom\u00e1tica de pre-commit a la rama <code>main</code>.</li> </ul> <p>C\u00f3mo configurar el secrets.PAT:</p> <ul> <li>Crear en github un Personal Access Token (PAT), para el repositorio espec\u00edfico. C\u00f3mo</li> <li>Darle acceso de lectura/escritura a \"Contents\", \"Pull Requests\" y \"Workflows\" en la secci\u00f3n \"Repository Permissions\".</li> <li>Agregar el PAT a los secretos del repositorio. Ir a la configuraci\u00f3n del repositorio &gt; Secrets and variables &gt; Actions. Luego en Repository secrets agregar un nuevo secreto de repositorio y nombrarlo <code>PAT</code> y pegar el token.</li> <li>Debes permitir expl\u00edcitamente a GitHub Actions crear pull requests. Esta configuraci\u00f3n se encuentra en la configuraci\u00f3n del repositorio bajo Actions &gt; General &gt; Workflow permissions. Seleccionar <code>Read repository contents and packages permissions</code></li> </ul>"},{"location":"es/setup_tokens/#codecov_token","title":"CODECOV_TOKEN","text":"<p>Este es el token para codecov. Se usa para subir el reporte de cobertura a codecov. Puedes obtenerlo desde codecov.io. No es requerido para desarrollo local. https://docs.codecov.com/docs/quick-start</p> <p>Debes agregar este secreto al repositorio de github. C\u00f3mo agregar codecov al repositorio de github: https://docs.codecov.com/docs/adding-the-codecov-token#github-actions</p>"},{"location":"es/setup_tokens/#referencias","title":"Referencias","text":"<ul> <li>https://github.com/peter-evans/create-pull-request?tab=readme-ov-file#workflow-permissions</li> <li>https://github.com/peter-evans/create-pull-request/issues/2443</li> <li>https://docs.github.com/en/authentication/keeping-your-account-and-data-secure/managing-your-personal-access-tokens#creating-a-fine-grained-personal-access-token</li> </ul>"},{"location":"es/vscode/","title":"Configuraci\u00f3n de VSCode","text":"<p>Las siguientes son algunas configuraciones comunes que pueden ser configuradas en Visual Studio Code para mejorar la experiencia de desarrollo en Python.</p> <p>https://github.com/JoseRZapata/data-science-project-template/.vscode/settings.json</p> <pre><code>{\n  \"[python]\": {\n\n    \"editor.codeActionsOnSave\": {\n      \"source.fixAll.ruff\": \"explicit\",\n      \"source.organizeImports.ruff\": \"explicit\",\n    },\n    \"editor.formatOnSave\": true,\n        \"editor.rulers\": [\n      100\n    ]\n  },\n  \"files.exclude\": {\n    \"**/__pycache__\": true\n  },\n  \"python.languageServer\": \"Pylance\",\n  \"editor.formatOnPaste\": true,\n  \"notebook.lineNumbers\": \"on\",\n  \"editor.inlineSuggest.enabled\": true,\n  \"editor.formatOnType\": true,\n  \"git.autofetch\": true,\n  \"editor.defaultFormatter\": \"charliermarsh.ruff\",\n  \"python.terminal.activateEnvInCurrentTerminal\": true,\n}\n</code></pre> <ul> <li> <p><code>[python]</code>: Esta secci\u00f3n aplica configuraciones espec\u00edficamente para archivos Python.</p> </li> <li> <p><code>\"editor.codeActionsOnSave\"</code>: Especifica acciones a realizar cuando se guarda un archivo Python.</p> </li> <li><code>\"source.fixAll.ruff\"</code>: \"explicit\": La funci\u00f3n de auto-correcci\u00f3n de Ruff est\u00e1 en modo expl\u00edcito, lo que significa que solo corregir\u00e1 problemas cuando se le indique expl\u00edcitamente.</li> <li><code>\"source.organizeImports.ruff\": \"explicit\"</code>: La funci\u00f3n de organizaci\u00f3n de imports de Ruff est\u00e1 en modo expl\u00edcito, lo que significa que solo organizar\u00e1 imports cuando se le indique expl\u00edcitamente.</li> <li><code>\"editor.formatOnSave\": true</code>: Esta configuraci\u00f3n habilita el formateo autom\u00e1tico de c\u00f3digo cuando se guarda un archivo Python.</li> <li><code>\"editor.rulers\": [100]</code>: Esta configuraci\u00f3n agrega una regla vertical en el car\u00e1cter 100 del editor para archivos Python como gu\u00eda de longitud de l\u00ednea.</li> <li> <p><code>\"files.exclude\": {\"**/__pycache__\": true}</code>: Esta configuraci\u00f3n oculta todos los directorios pycache en el explorador de archivos.</p> </li> <li> <p><code>\"python.languageServer\": \"Pylance\"</code>: Esta configuraci\u00f3n especifica Pylance como el servidor de lenguaje para Python. Un servidor de lenguaje proporciona caracter\u00edsticas como autocompletado y resaltado de sintaxis.</p> </li> <li> <p><code>\"editor.formatOnPaste\": true</code>: Esta configuraci\u00f3n habilita el formateo autom\u00e1tico de c\u00f3digo cuando pegas c\u00f3digo en el editor.</p> </li> <li> <p><code>\"notebook.lineNumbers\": \"on\"</code>: Esta configuraci\u00f3n habilita los n\u00fameros de l\u00ednea en Jupyter notebooks.</p> </li> <li> <p><code>\"editor.inlineSuggest.enabled\": true</code>: Esta configuraci\u00f3n habilita las sugerencias en l\u00ednea, que muestran completados sugeridos mientras escribes.</p> </li> <li> <p><code>\"editor.formatOnType\": true</code>: Esta configuraci\u00f3n habilita el formateo autom\u00e1tico de c\u00f3digo mientras escribes.</p> </li> <li> <p><code>\"git.autofetch\": true</code>: Esta configuraci\u00f3n habilita la obtenci\u00f3n autom\u00e1tica de datos de Git.</p> </li> <li> <p><code>\"editor.defaultFormatter\": \"charliermarsh.ruff\"</code>: Esta configuraci\u00f3n especifica Ruff como el formateador predeterminado para c\u00f3digo en el editor.</p> </li> <li> <p><code>\"python.terminal.activateEnvInCurrentTerminal\": true</code>: Esta configuraci\u00f3n habilita la activaci\u00f3n autom\u00e1tica del entorno Python en la terminal actual.</p> </li> <li> <p><code>\"python.defaultInterpreterPath\": \"${workspaceFolder}/.venv\"</code>: Esta configuraci\u00f3n especifica la ruta predeterminada del int\u00e9rprete de Python al entorno virtual creado en el proyecto.</p> </li> </ul>"},{"location":"es/github_actions/automerge/","title":"GitHub Action: Automerge","text":"<p>Este flujo de trabajo de GitHub Action automatiza la fusi\u00f3n de pull requests etiquetados con <code>automerge</code>. Soporta ejecuciones programadas y ejecuci\u00f3n manual, asegurando una integraci\u00f3n \u00e1gil de cambios que cumplan con criterios predefinidos.</p> <p>automerge.yml</p>"},{"location":"es/github_actions/automerge/#detalles-del-flujo-de-trabajo","title":"Detalles del Flujo de Trabajo","text":""},{"location":"es/github_actions/automerge/#disparadores-on","title":"Disparadores (<code>on</code>)","text":"<ul> <li><code>schedule</code>:<ul> <li>Se ejecuta autom\u00e1ticamente diariamente a las 08:00 UTC (seg\u00fan el cron schedule: <code>0 8 * * *</code>).</li> </ul> </li> <li><code>workflow_dispatch</code>:<ul> <li>Permite la ejecuci\u00f3n manual del flujo de trabajo a trav\u00e9s de la interfaz de GitHub Actions.</li> </ul> </li> </ul>"},{"location":"es/github_actions/automerge/#detalles-del-job","title":"Detalles del Job","text":""},{"location":"es/github_actions/automerge/#automerge","title":"automerge","text":"<p><code>runs-on</code>: <code>ubuntu-latest</code></p> <p>Pasos:</p> <ol> <li>Automerge:</li> <li>Usa la acci\u00f3n <code>pascalgn/automerge-action@v0.16.3</code> para fusionar pull requests que cumplan criterios espec\u00edficos.</li> <li>Variables de Entorno:<ul> <li><code>GITHUB_TOKEN</code>: Un token de acceso personal (PAT) almacenado en los secretos del repositorio para autenticaci\u00f3n.</li> <li><code>MERGE_METHOD</code>: Especifica el m\u00e9todo de fusi\u00f3n. En este caso, se usa el m\u00e9todo <code>squash</code>, que combina todos los commits en un solo commit al fusionar.</li> <li><code>MERGE_REQUIRED_APPROVALS</code>: Define el n\u00famero de aprobaciones requeridas para la fusi\u00f3n. Aqu\u00ed est\u00e1 configurado en <code>\"0\"</code>, lo que significa que no se requieren aprobaciones.</li> <li><code>MERGE_LABELS</code>: Especifica la etiqueta (<code>automerge</code>) que activa el proceso de automerge.</li> </ul> </li> </ol> <p>En resumen, este flujo de trabajo simplifica el proceso de fusi\u00f3n de pull requests al fusionar autom\u00e1ticamente aquellos etiquetados con <code>automerge</code> usando el m\u00e9todo <code>squash</code>. Puede ejecutarse diariamente o manualmente, asegurando flexibilidad y eficiencia en el proceso de desarrollo.</p>"},{"location":"es/github_actions/ci/","title":"GitHub Action: Pruebas CI/CD","text":"<p>Este flujo de trabajo de GitHub Action est\u00e1 dise\u00f1ado para optimizar y automatizar los procesos de CI/CD de tu proyecto. El flujo de trabajo se activa en pull requests y pushes a la rama <code>main</code>. Realiza varias tareas clave incluyendo linting, verificaciones de pre-commit, validaci\u00f3n de actualizaci\u00f3n del proyecto y ejecuci\u00f3n de pruebas con reportes de cobertura.</p> <p>ci.yml</p>"},{"location":"es/github_actions/ci/#detalles-del-flujo-de-trabajo","title":"Detalles del Flujo de Trabajo","text":""},{"location":"es/github_actions/ci/#disparadores-on","title":"Disparadores (<code>on</code>)","text":"<ul> <li><code>pull_request</code>: El flujo de trabajo se ejecuta cuando se abre o actualiza un pull request.</li> <li><code>push</code>: El flujo de trabajo se activa para pushes a la rama <code>main</code>.</li> </ul>"},{"location":"es/github_actions/ci/#resumen-de-jobs","title":"Resumen de Jobs","text":"<p>El flujo de trabajo define cuatro jobs:</p> <ol> <li><code>actionlint</code>: Valida la sintaxis y estructura de los archivos de flujo de trabajo de GitHub Action.</li> <li><code>lint-cruft</code>: Asegura que no existan archivos <code>.rej</code>, verificando que las actualizaciones del proyecto se aplicaron correctamente.</li> <li><code>pre-commit</code>: Ejecuta los hooks de pre-commit para aplicar est\u00e1ndares de c\u00f3digo y formateo en todos los archivos.</li> <li><code>test</code>: Ejecuta pruebas unitarias con reportes de cobertura y sube los resultados a Codecov.</li> </ol>"},{"location":"es/github_actions/ci/#detalles-de-los-jobs","title":"Detalles de los Jobs","text":""},{"location":"es/github_actions/ci/#1-actionlint","title":"1. actionlint","text":"<p><code>runs-on</code>: <code>ubuntu-latest</code></p> <p>Pasos:</p> <ul> <li>Checkout: Usa <code>actions/checkout@v4</code> para obtener el repositorio.</li> <li>Descargar actionlint: Obtiene <code>actionlint</code> usando un script bash.</li> <li>Verificar archivos de flujo de trabajo: Ejecuta <code>actionlint</code> para validar los archivos de flujo de trabajo.</li> </ul>"},{"location":"es/github_actions/ci/#2-lint-cruft","title":"2. lint-cruft","text":"<p><code>runs-on</code>: <code>ubuntu-latest</code></p> <p>Pasos:</p> <ul> <li>Checkout: Usa <code>actions/checkout@v4</code> para obtener el repositorio.</li> <li>Verificar archivos <code>.rej</code>: Falla el job si se encuentran archivos <code>.rej</code>, indicando una actualizaci\u00f3n no exitosa de la estructura del proyecto.</li> </ul>"},{"location":"es/github_actions/ci/#3-pre-commit","title":"3. pre-commit","text":"<p><code>runs-on</code>: <code>ubuntu-latest</code></p> <p>Pasos:</p> <ul> <li>Checkout: Usa <code>actions/checkout@v4</code> para obtener el repositorio.</li> <li>Instalar <code>uv</code>: Configura la herramienta <code>uv</code> usando <code>astral-sh/setup-uv@v6</code>.</li> <li>Ejecutar hooks de pre-commit: Ejecuta todos los hooks de pre-commit en todo el repositorio, mostrando cualquier fallo con diffs coloreados.</li> </ul>"},{"location":"es/github_actions/ci/#4-test","title":"4. test","text":"<p><code>runs-on</code>: <code>ubuntu-latest</code></p> <p>Pasos:</p> <ul> <li>Checkout: Usa <code>actions/checkout@v4</code> para obtener el repositorio.</li> <li>Instalar <code>uv</code>: Configura la herramienta <code>uv</code> usando <code>astral-sh/setup-uv@v6</code>.</li> <li>Configurar Python: Configura Python seg\u00fan el archivo <code>.python-version</code> usando <code>actions/setup-python@v5</code>.</li> <li>Instalar el proyecto: Instala el proyecto con todos los extras y dependencias de desarrollo usando <code>uv sync</code>.</li> <li>Ejecutar pruebas con cobertura: Ejecuta pruebas con <code>pytest</code> y genera un reporte de cobertura en formato XML.</li> <li>Subir reporte de cobertura: Sube el reporte de cobertura a Codecov usando <code>codecov/codecov-action@v5</code>. Requiere un secreto <code>CODECOV_TOKEN</code> almacenado en la configuraci\u00f3n del repositorio.</li> </ul> <p>En resumen, este flujo de trabajo automatiza el linting, la validaci\u00f3n de actualizaci\u00f3n del proyecto, las verificaciones de pre-commit, las pruebas y los reportes de cobertura para un pipeline de CI/CD robusto.</p>"},{"location":"es/github_actions/dependency_review/","title":"GitHub Action: Revisi\u00f3n de Dependencias","text":"<p>Este flujo de trabajo de GitHub Action est\u00e1 dise\u00f1ado para revisar autom\u00e1ticamente los cambios en las dependencias de tu proyecto durante los pull requests. Usa la acci\u00f3n <code>dependency-review-action</code> de GitHub para analizar el grafo de dependencias y se\u00f1alar posibles problemas de seguridad o cambios no deseados en las dependencias.</p> <p>dependency-review.yml</p>"},{"location":"es/github_actions/dependency_review/#detalles-del-flujo-de-trabajo","title":"Detalles del Flujo de Trabajo","text":""},{"location":"es/github_actions/dependency_review/#disparadores-on","title":"Disparadores (<code>on</code>)","text":"<ul> <li><code>pull_request</code>: El flujo de trabajo se activa cada vez que se crea o actualiza un pull request, asegurando que las revisiones de dependencias se realicen como parte del proceso de revisi\u00f3n de c\u00f3digo.</li> </ul>"},{"location":"es/github_actions/dependency_review/#permisos","title":"Permisos","text":"<ul> <li><code>contents: read</code>: Otorga al flujo de trabajo acceso de solo lectura al contenido del repositorio, lo cual es suficiente para realizar la revisi\u00f3n de dependencias.</li> </ul>"},{"location":"es/github_actions/dependency_review/#detalles-del-job","title":"Detalles del Job","text":""},{"location":"es/github_actions/dependency_review/#dependency-review","title":"dependency-review","text":"<p><code>runs-on</code>: <code>ubuntu-latest</code></p> <p>Pasos:</p> <ol> <li>Checkout del repositorio:</li> <li>Usa <code>actions/checkout@v4</code> para obtener el c\u00f3digo del pull request para an\u00e1lisis.</li> <li>Revisi\u00f3n de Dependencias:</li> <li>Usa <code>actions/dependency-review-action@v4</code> para analizar cambios en las dependencias. Esta acci\u00f3n verifica:<ul> <li>Dependencias reci\u00e9n agregadas.</li> <li>Actualizaciones a dependencias existentes.</li> <li>Vulnerabilidades de seguridad en cualquier dependencia afectada.</li> </ul> </li> </ol> <p>En resumen, el flujo de trabajo \"Revisi\u00f3n de Dependencias\" mejora la seguridad de tu proyecto al evaluar autom\u00e1ticamente los cambios de dependencias en los pull requests, ayud\u00e1ndote a detectar posibles problemas temprano en el ciclo de desarrollo.</p>"},{"location":"es/github_actions/docs/","title":"GitHub Action: Construcci\u00f3n y Despliegue de Documentaci\u00f3n","text":"<p>Este flujo de trabajo de GitHub Action automatiza el proceso de construcci\u00f3n y despliegue de la documentaci\u00f3n del proyecto usando <code>mkdocs</code>. Asegura que las actualizaciones de documentaci\u00f3n se publiquen autom\u00e1ticamente cada vez que se fusionen pull requests que afecten archivos espec\u00edficos o cuando se active manualmente.</p> <p>docs.yml</p>"},{"location":"es/github_actions/docs/#detalles-del-flujo-de-trabajo","title":"Detalles del Flujo de Trabajo","text":""},{"location":"es/github_actions/docs/#disparadores-on","title":"Disparadores (<code>on</code>)","text":"<ul> <li><code>pull_request</code>:<ul> <li>Se activa cuando un pull request que afecta archivos de documentaci\u00f3n se cierra (fusiona) en la rama <code>main</code>.</li> <li>Rutas monitoreadas:<ul> <li>Archivos en el directorio <code>docs/</code>.</li> <li><code>readme.md</code>.</li> </ul> </li> </ul> </li> <li><code>workflow_dispatch</code>: Permite la activaci\u00f3n manual del flujo de trabajo a trav\u00e9s de la interfaz de usuario de GitHub.</li> </ul>"},{"location":"es/github_actions/docs/#detalles-del-job","title":"Detalles del Job","text":""},{"location":"es/github_actions/docs/#build-and-publish","title":"build-and-publish","text":"<p><code>runs-on</code>: <code>ubuntu-latest</code></p> <p>Pasos:</p> <ol> <li>Checkout del repositorio:</li> <li> <p>Usa <code>actions/checkout@v4</code> para obtener el contenido del repositorio, asegurando acceso a los archivos de documentaci\u00f3n m\u00e1s recientes.</p> </li> <li> <p>Instalar <code>uv</code>:</p> </li> <li> <p>Configura la herramienta <code>uv</code> usando <code>astral-sh/setup-uv@v6</code>.</p> </li> <li> <p>Configurar Python:</p> </li> <li> <p>Configura Python seg\u00fan el archivo <code>.python-version</code> usando <code>actions/setup-python@v5</code>.</p> </li> <li> <p>Construir documentaci\u00f3n:</p> </li> <li> <p>Ejecuta <code>mkdocs build --clean</code> mediante <code>uv</code> para generar el sitio de documentaci\u00f3n est\u00e1tico. La opci\u00f3n <code>--clean</code> asegura que el directorio de salida se limpie antes de construir.</p> </li> <li> <p>Desplegar documentaci\u00f3n:</p> </li> <li>Usa <code>peaceiris/actions-gh-pages@v4</code> para publicar la documentaci\u00f3n construida en GitHub Pages.</li> <li>Entradas:<ul> <li><code>github_token</code>: Un Token de Acceso Personal (PAT) almacenado como secreto del repositorio.</li> <li><code>publish_dir</code>: Especifica el directorio que contiene el sitio construido (<code>./site</code>).</li> </ul> </li> </ol> <p>En resumen, este flujo de trabajo optimiza el proceso de construcci\u00f3n y despliegue de documentaci\u00f3n. Actualiza autom\u00e1ticamente GitHub Pages cada vez que se fusionan archivos relacionados con documentaci\u00f3n en la rama principal, con la flexibilidad adicional de activaci\u00f3n manual para actualizaciones bajo demanda.</p>"},{"location":"es/github_actions/gh_action_pre-commit-autoupdate/","title":"GitHub Action: Actualizaci\u00f3n autom\u00e1tica de Pre-commit","text":"<p>Este archivo de GitHub Action configura un flujo de trabajo llamado \"Pre-commit auto-update\". Este flujo de trabajo es responsable de actualizar autom\u00e1ticamente los hooks de pre-commit en tu repositorio.</p> <p>pre-commit_autoupdate.yml</p>"},{"location":"es/github_actions/gh_action_pre-commit-autoupdate/#detalles-del-flujo-de-trabajo","title":"Detalles del Flujo de Trabajo","text":"<ul> <li> <p><code>on</code>: Este flujo de trabajo se activa en dos situaciones:</p> <ul> <li><code>schedule</code>: Se ejecuta autom\u00e1ticamente cada lunes a las 7:00 UTC (seg\u00fan el cron schedule <code>\"0 7 * * 1\"</code>).</li> <li><code>workflow_dispatch</code>: Permite ejecutar manualmente el flujo de trabajo desde la interfaz de usuario de GitHub.</li> <li><code>jobs</code>: Define un job llamado \"pre-commit-update\".</li> </ul> </li> </ul>"},{"location":"es/github_actions/gh_action_pre-commit-autoupdate/#detalles-del-job-pre-commit-update","title":"Detalles del Job \"pre-commit-update\"","text":"<p><code>runs-on</code>: Este job se ejecuta en la \u00faltima versi\u00f3n de Ubuntu.</p> <p><code>steps</code>: Define los pasos a seguir en este job.</p> <p><code>Checkout</code>: Este paso usa la acci\u00f3n actions/checkout@v3 para obtener una copia del repositorio.</p> <p><code>Actualizar hooks de pre-commit</code>: Este paso usa la acci\u00f3n <code>brokenpip3/action-pre-commit-update</code> para actualizar los hooks de pre-commit. Esta acci\u00f3n requiere un token de GitHub para funcionar, que se pasa mediante <code>github-token: ${{ secrets.PAT }}</code>. PAT es un secreto almacenado en la configuraci\u00f3n del repositorio que contiene un token de acceso personal con alcance de repositorio.</p> <p>En resumen, este flujo de trabajo se encarga de mantener actualizados los hooks de pre-commit en el repositorio, ejecut\u00e1ndose autom\u00e1ticamente cada lunes y permitiendo tambi\u00e9n la ejecuci\u00f3n manual cuando sea necesario.</p>"},{"location":"es/github_actions/labels/","title":"GitHub Action: Sincronizaci\u00f3n de Etiquetas","text":"<p>Este flujo de trabajo de GitHub Action automatiza la sincronizaci\u00f3n de etiquetas de GitHub bas\u00e1ndose en un archivo de configuraci\u00f3n YAML. Se ejecuta ante cambios en la configuraci\u00f3n de etiquetas o en su archivo de flujo de trabajo y asegura consistencia en la gesti\u00f3n de etiquetas en todo el repositorio.</p> <p>github-labeler.yml</p>"},{"location":"es/github_actions/labels/#detalles-del-flujo-de-trabajo","title":"Detalles del Flujo de Trabajo","text":""},{"location":"es/github_actions/labels/#disparadores-on","title":"Disparadores (<code>on</code>)","text":"<ul> <li><code>push</code>:<ul> <li>Se activa cuando se hacen pushes a la rama <code>main</code>.</li> <li>Rutas monitoreadas:<ul> <li><code>.github/labels.yml</code>: El archivo YAML que contiene las definiciones de etiquetas.</li> <li><code>.github/workflows/labels.yml</code>: La configuraci\u00f3n del flujo de trabajo para sincronizaci\u00f3n de etiquetas.</li> </ul> </li> </ul> </li> <li><code>pull_request</code>:<ul> <li>Se activa cuando los pull requests modifican los archivos <code>.github/labels.yml</code> o <code>.github/workflows/labels.yml</code>.</li> </ul> </li> </ul>"},{"location":"es/github_actions/labels/#detalles-del-job","title":"Detalles del Job","text":""},{"location":"es/github_actions/labels/#labeler","title":"labeler","text":"<p><code>runs-on</code>: <code>ubuntu-latest</code></p> <p>Pasos:</p> <ol> <li>Checkout del repositorio:</li> <li> <p>Usa <code>actions/checkout@v4</code> para obtener el contenido del repositorio.</p> </li> <li> <p>Ejecutar Labeler:</p> </li> <li>Usa <code>crazy-max/ghaction-github-labeler@v5</code> para sincronizar etiquetas bas\u00e1ndose en el archivo <code>.github/labels.yml</code>.</li> <li>Entradas:<ul> <li><code>github-token</code>: Un token de acceso personal (PAT) almacenado en los secretos del repositorio para autenticaci\u00f3n.</li> <li><code>yaml-file</code>: Especifica la ruta al archivo de configuraci\u00f3n de etiquetas (<code>.github/labels.yml</code>).</li> <li><code>dry-run</code>: Simula el proceso de sincronizaci\u00f3n de etiquetas cuando el flujo de trabajo se activa por un pull request (<code>true</code> para pull requests, <code>false</code> en caso contrario).</li> <li><code>exclude</code>: Excluye etiquetas espec\u00edficas de la sincronizaci\u00f3n. Aqu\u00ed, las etiquetas que coincidan con <code>help*</code> o <code>*issue</code> son excluidas.</li> </ul> </li> </ol> <p>En resumen, este flujo de trabajo asegura que las etiquetas de GitHub en tu repositorio se mantengan consistentes con las definiciones en <code>.github/labels.yml</code>. Soporta tanto actualizaciones en vivo para pushes como modo de simulaci\u00f3n para pull requests, proporcionando una soluci\u00f3n robusta para la gesti\u00f3n de etiquetas.</p>"}]}